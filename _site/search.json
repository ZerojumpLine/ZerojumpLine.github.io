[
  {
    "objectID": "alumni/index.html",
    "href": "alumni/index.html",
    "title": "Alumni",
    "section": "",
    "text": "Order By Name Order By Role Order By Start Date Order By End Date \n\n\n\n\n\n\n\n\n\n\nName\n\n\nRole/Affiliation\n\n\nStarted\n\n\nEnded\n\n\n\n\n\n\n Bowen Guo\n\n\nUG Student, Fudan University\n\n\nFeb 2025\n\n\nJul 2025\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/2025-08-08-Undergraduate Project/index.html",
    "href": "teaching/2025-08-08-Undergraduate Project/index.html",
    "title": "Undergraduate Project 2025",
    "section": "",
    "text": "I am teaching a practical course for BME senior undergraduate students."
  },
  {
    "objectID": "teaching/2025-08-08-Undergraduate Project/index.html#medical-llms",
    "href": "teaching/2025-08-08-Undergraduate Project/index.html#medical-llms",
    "title": "Undergraduate Project 2025",
    "section": "Medical LLMs",
    "text": "Medical LLMs\nThis semester, we will investigate the use of LLMs in medical applications."
  },
  {
    "objectID": "teaching/2025-08-08-Undergraduate Project/index.html#recommended-reading-materials",
    "href": "teaching/2025-08-08-Undergraduate Project/index.html#recommended-reading-materials",
    "title": "Undergraduate Project 2025",
    "section": "Recommended Reading Materials",
    "text": "Recommended Reading Materials\n\nMathematics for Machine Learning.\nDive into Deep Learning."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "AI Foundations 2025\n\n\n\nUG\n\nGeneral education course\n\n\n\n\nSep 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPattern Recognition 2025\n\n\n\nPh.D.\n\nBME\n\n\n\n\nSep 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nUndergraduate Project 2025\n\n\n\nUG\n\nBME\n\n\n\n\nSep 1, 2025\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "team/danqing_gu/index.html",
    "href": "team/danqing_gu/index.html",
    "title": "Danqing Gu",
    "section": "",
    "text": "È°æ‰∏πÈùí\n\nÁ°ïÂ£´ÁîüÁ†îÁ©∂Âëò\n¬†\nCollege of Biomedical Engineering\nFudan University\n¬†\nLab Period\nSep 2024 - Present\nüìß Email 24210720032(at)m.fudan.edu.cn"
  },
  {
    "objectID": "team/danqing_gu/index.html#about",
    "href": "team/danqing_gu/index.html#about",
    "title": "Danqing Gu",
    "section": "About",
    "text": "About\n\n\nDanqing Gu is a master‚Äôs student working on counterfactual image generation.\n\n\nDanqing joined the lab in 2024 and is currently pursuing his Master‚Äôs degree in Biomedical Engineering at the Fudan University.\n\n\nM.S. ‚àô Fudan University ‚àô Expected 2027\n\n\nB.S. ‚àô Fudan University ‚àô 2024"
  },
  {
    "objectID": "team/jiahe_wang/index.html",
    "href": "team/jiahe_wang/index.html",
    "title": "Jiahe Wang",
    "section": "",
    "text": "ÁéãÂòâÁ¶æ\n\nÂÆû‰π†Á†îÁ©∂Âëò\n¬†\nDepartment of Biomedical Engineering\nTulane University of Louisiana\n¬†\nLab Period\nJun 2025 - Present\nüìß Email jwang82(at)tulane.edu"
  },
  {
    "objectID": "team/jiahe_wang/index.html#about",
    "href": "team/jiahe_wang/index.html#about",
    "title": "Jiahe Wang",
    "section": "About",
    "text": "About\n\n\nJiahe Wang is a research intern working on building a foundation model for ultrasound imaging.\n\n\nJiahe joined the lab in 2025 and is currently pursuing her Bachelor‚Äôs degree in Biomedical Engineering at Tulane University of Louisiana.\n\n\nB.S. ‚àô Tulane University of Louisiana ‚àô Expected 2027"
  },
  {
    "objectID": "team/haoyan_ma/index.html",
    "href": "team/haoyan_ma/index.html",
    "title": "Haoyan Ma",
    "section": "",
    "text": "È©¨ÁÅèÁÑ±\n\nÁ°ïÂ£´ÁîüÁ†îÁ©∂Âëò\n¬†\nCollege of Biomedical Engineering\nFudan University\n¬†\nLab Period\nMay 2025 - Present\nüìß Email maxsim_ma(at)163.com"
  },
  {
    "objectID": "team/haoyan_ma/index.html#about",
    "href": "team/haoyan_ma/index.html#about",
    "title": "Haoyan Ma",
    "section": "About",
    "text": "About\n\n\nHaoyan Ma is a master‚Äôs student working on privacy problems in generative models.\n\n\nHaoyan joined the lab in 2025 and is currently pursuing his Master‚Äôs degree in Biomedical Engineering at the Fudan University.\n\n\nM.S. ‚àô Fudan University ‚àô Expected 2028\n\n\nB.S. ‚àô Fudan University ‚àô 2025"
  },
  {
    "objectID": "team/jiaheng_dai/index.html",
    "href": "team/jiaheng_dai/index.html",
    "title": "Jiaheng Dai",
    "section": "",
    "text": "Êà¥ÂòâÊÅí\n\nÂçöÂ£´ÁîüÁ†îÁ©∂Âëò\n¬†\nCollege of Biomedical Engineering\nFudan University\n¬†\nLab Period\nSep 2025 - Present\nüìß Email 25113060007(at)m.fudan.edu.cn"
  },
  {
    "objectID": "team/jiaheng_dai/index.html#about",
    "href": "team/jiaheng_dai/index.html#about",
    "title": "Jiaheng Dai",
    "section": "About",
    "text": "About\n\n\nJiaheng Dai is a doctor student working on medical image analysis.\n\n\nJiaheng joined the lab in 2025 and is currently pursuing his doctoral degree in Biomedical Engineering at Fudan University.\n\n\nPh.D.¬†‚àô Fudan University ‚àô Expected 2029\n\n\nB.S. ‚àô University of Edinburgh ‚àô 2025\n\n\nB.S. ‚àô University of Edinburgh ‚àô 2024"
  },
  {
    "objectID": "team/index.html",
    "href": "team/index.html",
    "title": "Team",
    "section": "",
    "text": "We are a part of research team lead by Prof.¬†Yuanyuan Wang. I feel truly fortunate to collaborate with such a talented and kind team."
  },
  {
    "objectID": "team/index.html#current-team",
    "href": "team/index.html#current-team",
    "title": "Team",
    "section": "",
    "text": "We are a part of research team lead by Prof.¬†Yuanyuan Wang. I feel truly fortunate to collaborate with such a talented and kind team."
  },
  {
    "objectID": "team/index.html#alumni",
    "href": "team/index.html#alumni",
    "title": "Team",
    "section": "Alumni",
    "text": "Alumni\nOur lab has been fortunate to work with many talented researchers who have moved on to exciting opportunities. Learn more about our alumni and their current positions.\n\nVIEW ALUMNI DIRECTORY ‚Üí"
  },
  {
    "objectID": "team/jie_xu/index.html",
    "href": "team/jie_xu/index.html",
    "title": "Jie Xu",
    "section": "",
    "text": "ÂæêÊù∞\n\nÂçöÂ£´ÁîüÁ†îÁ©∂Âëò\n¬†\nCollege of Biomedical Engineering\nFudan University\n¬†\nLab Period\nSep 2023 - Present\nüìß Email jie_xu18(at)fudan.edu.cn"
  },
  {
    "objectID": "team/jie_xu/index.html#about",
    "href": "team/jie_xu/index.html#about",
    "title": "Jie Xu",
    "section": "About",
    "text": "About\n\n\nJie Xu is a doctor student working on resource efficient learning.\n\n\nJie joined the lab in 2023 and is currently pursuing his doctoral degree in Biomedical Engineering at Fudan University.\n\n\nPh.D.¬†‚àô Fudan University ‚àô Expected 2028\n\n\nB.S. ‚àô Fudan University ‚àô 2023"
  },
  {
    "objectID": "team/yuejian_wu/index.html",
    "href": "team/yuejian_wu/index.html",
    "title": "Yuejian Wu",
    "section": "",
    "text": "‰ºçÊ®æÁëä\n\nÂçöÂ£´ÁîüÁ†îÁ©∂Âëò\n¬†\nCollege of Biomedical Engineering\nFudan University\n¬†\nLab Period\nFeb 2025 - Present\nüìß Email 1337347541(at)qq.com"
  },
  {
    "objectID": "team/yuejian_wu/index.html#about",
    "href": "team/yuejian_wu/index.html#about",
    "title": "Yuejian Wu",
    "section": "About",
    "text": "About\n\n\nYuejian Wu is a doctor student working on multi-modal learning and class imabalance.\n\n\nYuejian joined the lab in 2025 and is currently pursuing his doctoral degree in Biomedical Engineering at Fudan University.\n\n\nPh.D.¬†‚àô Fudan University ‚àô Expected 2030\n\n\nB.S. ‚àô Fudan University ‚àô 2025"
  },
  {
    "objectID": "news/index.html",
    "href": "news/index.html",
    "title": "News",
    "section": "",
    "text": "Generative Machine Learning Models in Medical Image Computing\n\n\n\nBook\n\nGenAI\n\n\n\n\n\n\nAug 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDehazing Echocardiography Challenge 2025\n\n\n\nChallenge\n\nGenAI\n\n\n\n\n\n\nAug 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMICS 2025 at Cixi\n\n\n\nSeminar\n\nMedical Image Analysis\n\n\n\n\n\n\nAug 8, 2025\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "news/2025-08-08-GenAI-Book/index.html",
    "href": "news/2025-08-08-GenAI-Book/index.html",
    "title": "Generative Machine Learning Models in Medical Image Computing",
    "section": "",
    "text": "The book we wrote is available online at link."
  },
  {
    "objectID": "news/2025-08-08-GenAI-Book/index.html#description",
    "href": "news/2025-08-08-GenAI-Book/index.html#description",
    "title": "Generative Machine Learning Models in Medical Image Computing",
    "section": "",
    "text": "The book we wrote is available online at link."
  },
  {
    "objectID": "about/find-me.html",
    "href": "about/find-me.html",
    "title": "Find Me",
    "section": "",
    "text": "Zeju Li, PhD\nCollege of Biomedical Engineering\nFudan University\nRoom C2008, No.¬†2 Interdisciplinary Research Building\nJiangwan Campus, Fudan University\nShanghai, China"
  },
  {
    "objectID": "about/find-me.html#office-location",
    "href": "about/find-me.html#office-location",
    "title": "Find Me",
    "section": "",
    "text": "Zeju Li, PhD\nCollege of Biomedical Engineering\nFudan University\nRoom C2008, No.¬†2 Interdisciplinary Research Building\nJiangwan Campus, Fudan University\nShanghai, China"
  },
  {
    "objectID": "about/find-me.html#contact-information",
    "href": "about/find-me.html#contact-information",
    "title": "Find Me",
    "section": "Contact Information",
    "text": "Contact Information\nEmail: zejuli@fudan.edu.cn\nOffice Hours: Monday-Friday, 9am-5pm"
  },
  {
    "objectID": "about/find-me.html#mailing-address",
    "href": "about/find-me.html#mailing-address",
    "title": "Find Me",
    "section": "Mailing Address",
    "text": "Mailing Address\nSame as office location."
  },
  {
    "objectID": "about/find-me.html#directions",
    "href": "about/find-me.html#directions",
    "title": "Find Me",
    "section": "Directions",
    "text": "Directions\n\nCampus OverviewBuilding LocationLab Space\n\n\n\n\n\n\n\n\nJiangwan Campus, Fudan University\n\n\n\nThe Jiangwan Campus is located in the northeastern part of Shanghai, featuring modern research facilities and a beautiful academic environment.\n\n\n\nCampus View\n\n\nGetting Here: - Metro: Line 10 to Jiangwan Stadium Station (Ê±üÊπæ‰ΩìËÇ≤Âú∫Á´ô) - Bus: Routes 966, 537, 854, 942 - Address: 2005 Songhu Road, Yangpu District, Shanghai\n\n\n\n\n\n\n\n\n\n\nNo.¬†2 Interdisciplinary Research Building\n\n\n\nOur building houses cutting-edge research facilities and collaborative spaces for interdisciplinary research.\n\n\n\nBuilding View\n\n\nBuilding Features: - Modern research laboratories - Collaborative meeting spaces - Advanced equipment facilities - Secure access control\n\n\n\n\n\n\n\n\n\n\nRoom C2008 - Biomedical Engineering Lab\n\n\n\nOur dedicated research space where innovation meets collaboration.\n\n\n\nLab View\n\n\nLab Features: - State-of-the-art research equipment - Collaborative workspace - Data analysis stations - Meeting area for team discussions"
  },
  {
    "objectID": "about/find-me.html#social-media-professional-networks",
    "href": "about/find-me.html#social-media-professional-networks",
    "title": "Find Me",
    "section": "Social Media & Professional Networks",
    "text": "Social Media & Professional Networks\n\nUniversity Website: Fudan-BME\nLinkedIn: LinkedIn\nTwitter/X: @li_zeju\nGoogle Scholar: Zeju Li\nORCID: 0000-0002-4608-2959"
  },
  {
    "objectID": "publication/2022-09-01-estimating-model-performance-domain-shifts/index.html",
    "href": "publication/2022-09-01-estimating-model-performance-domain-shifts/index.html",
    "title": "Estimating Model Performance under Domain Shifts with Class-Specific Confidence Scores",
    "section": "",
    "text": "Domain shifts pose significant challenges for deploying medical image analysis models in clinical settings, where performance degradation can have serious consequences. Traditional approaches for estimating model performance under domain shifts often rely on global confidence scores, which may not accurately reflect performance across different classes. In this work, we propose a novel approach for estimating model performance under domain shifts using class-specific confidence scores.\nOur method introduces a class-specific confidence estimation framework that provides more granular performance predictions for each class. The approach consists of two main components: (1) a class-specific confidence estimator that learns to predict performance for individual classes, and (2) a domain shift detector that identifies when the model is operating in a shifted domain.\nWe evaluate our approach on multiple medical image segmentation datasets with known domain shifts, including cross-scanner and cross-institution scenarios. Experimental results demonstrate that our class-specific confidence scores provide more accurate performance estimates compared to global confidence measures. The method shows particular effectiveness for minority classes, which are often most affected by domain shifts.\nThe proposed framework represents a significant advancement in domain shift detection and performance estimation, providing more reliable confidence measures that could improve the safe deployment of medical AI systems."
  },
  {
    "objectID": "publication/2022-09-01-estimating-model-performance-domain-shifts/index.html#abstract",
    "href": "publication/2022-09-01-estimating-model-performance-domain-shifts/index.html#abstract",
    "title": "Estimating Model Performance under Domain Shifts with Class-Specific Confidence Scores",
    "section": "",
    "text": "Domain shifts pose significant challenges for deploying medical image analysis models in clinical settings, where performance degradation can have serious consequences. Traditional approaches for estimating model performance under domain shifts often rely on global confidence scores, which may not accurately reflect performance across different classes. In this work, we propose a novel approach for estimating model performance under domain shifts using class-specific confidence scores.\nOur method introduces a class-specific confidence estimation framework that provides more granular performance predictions for each class. The approach consists of two main components: (1) a class-specific confidence estimator that learns to predict performance for individual classes, and (2) a domain shift detector that identifies when the model is operating in a shifted domain.\nWe evaluate our approach on multiple medical image segmentation datasets with known domain shifts, including cross-scanner and cross-institution scenarios. Experimental results demonstrate that our class-specific confidence scores provide more accurate performance estimates compared to global confidence measures. The method shows particular effectiveness for minority classes, which are often most affected by domain shifts.\nThe proposed framework represents a significant advancement in domain shift detection and performance estimation, providing more reliable confidence measures that could improve the safe deployment of medical AI systems."
  },
  {
    "objectID": "publication/2020-12-17-overfitting-class-imbalance/index.html",
    "href": "publication/2020-12-17-overfitting-class-imbalance/index.html",
    "title": "Analyzing Overfitting under Class Imbalance in Neural Networks for Image Segmentation",
    "section": "",
    "text": "Class imbalance is a pervasive challenge in medical image segmentation, where certain anatomical structures or pathological regions may be significantly underrepresented in the training data. This imbalance often leads to overfitting, where neural networks memorize the training data rather than learning generalizable features. In this work, we present a comprehensive analysis of overfitting behavior under class imbalance conditions in medical image segmentation.\nOur study investigates the relationship between class imbalance ratios and overfitting patterns across different network architectures and training strategies. We identify key factors that contribute to overfitting, including the degree of class imbalance, network capacity, and regularization techniques. Through extensive experiments on multiple medical imaging datasets, we demonstrate that overfitting manifests differently across classes, with minority classes being particularly susceptible.\nWe propose several strategies to mitigate overfitting under class imbalance, including adaptive learning rates, class-specific regularization, and curriculum learning approaches. Our analysis provides insights into the fundamental mechanisms driving overfitting in imbalanced medical image segmentation tasks and offers practical guidelines for model development.\nThe findings from this work contribute to a better understanding of neural network behavior under class imbalance conditions and provide a foundation for developing more robust segmentation models in medical imaging applications."
  },
  {
    "objectID": "publication/2020-12-17-overfitting-class-imbalance/index.html#abstract",
    "href": "publication/2020-12-17-overfitting-class-imbalance/index.html#abstract",
    "title": "Analyzing Overfitting under Class Imbalance in Neural Networks for Image Segmentation",
    "section": "",
    "text": "Class imbalance is a pervasive challenge in medical image segmentation, where certain anatomical structures or pathological regions may be significantly underrepresented in the training data. This imbalance often leads to overfitting, where neural networks memorize the training data rather than learning generalizable features. In this work, we present a comprehensive analysis of overfitting behavior under class imbalance conditions in medical image segmentation.\nOur study investigates the relationship between class imbalance ratios and overfitting patterns across different network architectures and training strategies. We identify key factors that contribute to overfitting, including the degree of class imbalance, network capacity, and regularization techniques. Through extensive experiments on multiple medical imaging datasets, we demonstrate that overfitting manifests differently across classes, with minority classes being particularly susceptible.\nWe propose several strategies to mitigate overfitting under class imbalance, including adaptive learning rates, class-specific regularization, and curriculum learning approaches. Our analysis provides insights into the fundamental mechanisms driving overfitting in imbalanced medical image segmentation tasks and offers practical guidelines for model development.\nThe findings from this work contribute to a better understanding of neural network behavior under class imbalance conditions and provide a foundation for developing more robust segmentation models in medical imaging applications."
  },
  {
    "objectID": "publication/2018-09-01-left-ventricle-segmentation-optical-flow/index.html",
    "href": "publication/2018-09-01-left-ventricle-segmentation-optical-flow/index.html",
    "title": "Left Ventricle Segmentation via Optical-Flow-Net from Short-Axis Cine MRI: Preserving the Temporal Coherence of Cardiac Motion",
    "section": "",
    "text": "Cardiac MRI segmentation requires maintaining temporal coherence across the cardiac cycle, which is crucial for accurate assessment of cardiac function. Traditional segmentation methods often process each frame independently, failing to capture the temporal relationships that are essential for cardiac analysis. In this work, we propose a novel optical-flow-based approach for left ventricle segmentation that preserves temporal coherence.\nOur method introduces an optical-flow network that captures motion patterns across the cardiac cycle to guide segmentation. The framework consists of three key components: (1) an optical-flow estimation module that learns cardiac motion patterns, (2) a temporal coherence module that ensures smooth segmentation across frames, and (3) a segmentation network that leverages motion information for accurate delineation.\nWe evaluate our approach on short-axis cine MRI datasets with expert annotations. Experimental results demonstrate that our optical-flow-based approach significantly improves segmentation accuracy compared to frame-independent methods. The method shows excellent performance in maintaining temporal coherence and capturing cardiac motion patterns.\nThe proposed framework represents a significant advancement in cardiac image analysis, providing more accurate and temporally consistent segmentation that could improve cardiac function assessment and clinical diagnosis."
  },
  {
    "objectID": "publication/2018-09-01-left-ventricle-segmentation-optical-flow/index.html#abstract",
    "href": "publication/2018-09-01-left-ventricle-segmentation-optical-flow/index.html#abstract",
    "title": "Left Ventricle Segmentation via Optical-Flow-Net from Short-Axis Cine MRI: Preserving the Temporal Coherence of Cardiac Motion",
    "section": "",
    "text": "Cardiac MRI segmentation requires maintaining temporal coherence across the cardiac cycle, which is crucial for accurate assessment of cardiac function. Traditional segmentation methods often process each frame independently, failing to capture the temporal relationships that are essential for cardiac analysis. In this work, we propose a novel optical-flow-based approach for left ventricle segmentation that preserves temporal coherence.\nOur method introduces an optical-flow network that captures motion patterns across the cardiac cycle to guide segmentation. The framework consists of three key components: (1) an optical-flow estimation module that learns cardiac motion patterns, (2) a temporal coherence module that ensures smooth segmentation across frames, and (3) a segmentation network that leverages motion information for accurate delineation.\nWe evaluate our approach on short-axis cine MRI datasets with expert annotations. Experimental results demonstrate that our optical-flow-based approach significantly improves segmentation accuracy compared to frame-independent methods. The method shows excellent performance in maintaining temporal coherence and capturing cardiac motion patterns.\nThe proposed framework represents a significant advancement in cardiac image analysis, providing more accurate and temporally consistent segmentation that could improve cardiac function assessment and clinical diagnosis."
  },
  {
    "objectID": "publication/2017-10-01-low-grade-glioma-segmentation/index.html",
    "href": "publication/2017-10-01-low-grade-glioma-segmentation/index.html",
    "title": "Low-Grade Glioma Segmentation Based on CNN with Fully Connected CRF",
    "section": "",
    "text": "Accurate segmentation of low-grade gliomas is crucial for treatment planning and patient management, but remains challenging due to the subtle appearance differences between tumor and normal brain tissue. Traditional segmentation methods often struggle with the complex boundaries and heterogeneous appearance of gliomas. In this work, we propose a novel approach that combines Convolutional Neural Networks (CNN) with Fully Connected Conditional Random Fields (CRF) for low-grade glioma segmentation.\nOur framework consists of two main components: (1) a deep CNN that learns discriminative features for glioma detection and initial segmentation, and (2) a fully connected CRF that refines the segmentation boundaries by incorporating spatial consistency and appearance constraints. The CNN provides robust feature learning capabilities, while the CRF ensures smooth and anatomically plausible segmentation results.\nWe evaluate our method on a comprehensive dataset of low-grade glioma patients with expert annotations. Experimental results demonstrate that our approach significantly outperforms traditional segmentation methods and CNN-only approaches. The method shows excellent performance in handling the challenging aspects of glioma segmentation, including boundary ambiguity and tissue heterogeneity.\nThe proposed framework represents a significant advancement in glioma segmentation, providing more accurate and clinically relevant results that could improve treatment planning and patient outcomes."
  },
  {
    "objectID": "publication/2017-10-01-low-grade-glioma-segmentation/index.html#abstract",
    "href": "publication/2017-10-01-low-grade-glioma-segmentation/index.html#abstract",
    "title": "Low-Grade Glioma Segmentation Based on CNN with Fully Connected CRF",
    "section": "",
    "text": "Accurate segmentation of low-grade gliomas is crucial for treatment planning and patient management, but remains challenging due to the subtle appearance differences between tumor and normal brain tissue. Traditional segmentation methods often struggle with the complex boundaries and heterogeneous appearance of gliomas. In this work, we propose a novel approach that combines Convolutional Neural Networks (CNN) with Fully Connected Conditional Random Fields (CRF) for low-grade glioma segmentation.\nOur framework consists of two main components: (1) a deep CNN that learns discriminative features for glioma detection and initial segmentation, and (2) a fully connected CRF that refines the segmentation boundaries by incorporating spatial consistency and appearance constraints. The CNN provides robust feature learning capabilities, while the CRF ensures smooth and anatomically plausible segmentation results.\nWe evaluate our method on a comprehensive dataset of low-grade glioma patients with expert annotations. Experimental results demonstrate that our approach significantly outperforms traditional segmentation methods and CNN-only approaches. The method shows excellent performance in handling the challenging aspects of glioma segmentation, including boundary ambiguity and tissue heterogeneity.\nThe proposed framework represents a significant advancement in glioma segmentation, providing more accurate and clinically relevant results that could improve treatment planning and patient outcomes."
  },
  {
    "objectID": "publication/2023-09-01-post-deployment-adaptation-federated-learning/index.html",
    "href": "publication/2023-09-01-post-deployment-adaptation-federated-learning/index.html",
    "title": "Post-Deployment Adaptation with Access to Source Data via Federated Learning and Source-Target Remote Gradient Alignment",
    "section": "",
    "text": "Post-deployment adaptation is crucial for maintaining model performance when deployed in new environments or with changing data distributions. However, traditional adaptation approaches often require direct access to source data, which may not be available due to privacy or logistical constraints. In this work, we propose a federated learning approach for post-deployment adaptation that enables effective adaptation while preserving data privacy.\nOur method introduces a source-target remote gradient alignment framework that enables adaptation without sharing raw data. The framework consists of three key components: (1) a federated learning protocol that enables collaborative adaptation, (2) a remote gradient alignment mechanism that synchronizes adaptation across domains, and (3) a privacy-preserving adaptation module that maintains data security.\nWe evaluate our approach on multiple medical imaging datasets with domain shifts, including cross-scanner and cross-institution scenarios. Experimental results demonstrate that our federated adaptation approach significantly improves model performance in new environments compared to traditional methods. The method shows robust performance while maintaining strict privacy standards.\nThe proposed framework represents a significant advancement in privacy-preserving model adaptation, providing effective post-deployment adaptation that could improve model performance in clinical settings."
  },
  {
    "objectID": "publication/2023-09-01-post-deployment-adaptation-federated-learning/index.html#abstract",
    "href": "publication/2023-09-01-post-deployment-adaptation-federated-learning/index.html#abstract",
    "title": "Post-Deployment Adaptation with Access to Source Data via Federated Learning and Source-Target Remote Gradient Alignment",
    "section": "",
    "text": "Post-deployment adaptation is crucial for maintaining model performance when deployed in new environments or with changing data distributions. However, traditional adaptation approaches often require direct access to source data, which may not be available due to privacy or logistical constraints. In this work, we propose a federated learning approach for post-deployment adaptation that enables effective adaptation while preserving data privacy.\nOur method introduces a source-target remote gradient alignment framework that enables adaptation without sharing raw data. The framework consists of three key components: (1) a federated learning protocol that enables collaborative adaptation, (2) a remote gradient alignment mechanism that synchronizes adaptation across domains, and (3) a privacy-preserving adaptation module that maintains data security.\nWe evaluate our approach on multiple medical imaging datasets with domain shifts, including cross-scanner and cross-institution scenarios. Experimental results demonstrate that our federated adaptation approach significantly improves model performance in new environments compared to traditional methods. The method shows robust performance while maintaining strict privacy standards.\nThe proposed framework represents a significant advancement in privacy-preserving model adaptation, providing effective post-deployment adaptation that could improve model performance in clinical settings."
  },
  {
    "objectID": "publication/2022-10-01-tackling-long-tailed-category-distribution/index.html",
    "href": "publication/2022-10-01-tackling-long-tailed-category-distribution/index.html",
    "title": "Tackling Long-Tailed Category Distribution Under Domain Shifts",
    "section": "",
    "text": "Long-tailed category distribution combined with domain shifts presents significant challenges in medical image analysis, where rare conditions may be underrepresented and domain variations can further exacerbate the imbalance. Traditional approaches often struggle with both challenges simultaneously. In this work, we propose a novel approach for tackling long-tailed category distribution under domain shifts.\nOur method introduces a comprehensive framework that addresses both long-tailed distribution and domain shift challenges. The framework consists of three key components: (1) a balanced sampling strategy that addresses category imbalance, (2) a domain adaptation module that handles domain shifts, and (3) a unified training approach that optimizes for both challenges simultaneously.\nWe evaluate our approach on multiple medical imaging datasets with long-tailed distributions and domain shifts. Experimental results demonstrate that our method significantly improves performance on rare categories while maintaining robustness across domains. The approach shows particular effectiveness in handling the challenging combination of long-tailed distribution and domain shifts.\nThe proposed framework represents a significant advancement in handling complex real-world scenarios in medical imaging, providing more robust and balanced performance across different categories and domains."
  },
  {
    "objectID": "publication/2022-10-01-tackling-long-tailed-category-distribution/index.html#abstract",
    "href": "publication/2022-10-01-tackling-long-tailed-category-distribution/index.html#abstract",
    "title": "Tackling Long-Tailed Category Distribution Under Domain Shifts",
    "section": "",
    "text": "Long-tailed category distribution combined with domain shifts presents significant challenges in medical image analysis, where rare conditions may be underrepresented and domain variations can further exacerbate the imbalance. Traditional approaches often struggle with both challenges simultaneously. In this work, we propose a novel approach for tackling long-tailed category distribution under domain shifts.\nOur method introduces a comprehensive framework that addresses both long-tailed distribution and domain shift challenges. The framework consists of three key components: (1) a balanced sampling strategy that addresses category imbalance, (2) a domain adaptation module that handles domain shifts, and (3) a unified training approach that optimizes for both challenges simultaneously.\nWe evaluate our approach on multiple medical imaging datasets with long-tailed distributions and domain shifts. Experimental results demonstrate that our method significantly improves performance on rare categories while maintaining robustness across domains. The approach shows particular effectiveness in handling the challenging combination of long-tailed distribution and domain shifts.\nThe proposed framework represents a significant advancement in handling complex real-world scenarios in medical imaging, providing more robust and balanced performance across different categories and domains."
  },
  {
    "objectID": "publication/2023-06-27-context-label-learning/index.html",
    "href": "publication/2023-06-27-context-label-learning/index.html",
    "title": "Context Label Learning: Improving Background Class Representations in Semantic Segmentation",
    "section": "",
    "text": "Semantic segmentation in medical imaging often faces challenges with background class representation, which can significantly impact segmentation accuracy. Traditional approaches treat background as a single class, failing to capture the rich contextual information present in medical images. In this work, we propose Context Label Learning (CLL), a novel framework that improves background class representations by learning context-aware labels.\nOur approach introduces a context-aware labeling mechanism that dynamically adapts background representations based on the surrounding anatomical structures. The framework consists of two key components: (1) a context encoder that captures spatial relationships between foreground and background regions, and (2) a label refinement module that generates context-specific background labels.\nWe evaluate our method on multiple medical image segmentation datasets, including brain tumor segmentation and multi-organ segmentation. Experimental results demonstrate that CLL significantly improves segmentation accuracy, particularly for challenging cases where background context is crucial for accurate delineation.\nThe proposed framework provides a more principled approach to handling background classes in medical image segmentation, leading to improved performance and better clinical interpretability."
  },
  {
    "objectID": "publication/2023-06-27-context-label-learning/index.html#abstract",
    "href": "publication/2023-06-27-context-label-learning/index.html#abstract",
    "title": "Context Label Learning: Improving Background Class Representations in Semantic Segmentation",
    "section": "",
    "text": "Semantic segmentation in medical imaging often faces challenges with background class representation, which can significantly impact segmentation accuracy. Traditional approaches treat background as a single class, failing to capture the rich contextual information present in medical images. In this work, we propose Context Label Learning (CLL), a novel framework that improves background class representations by learning context-aware labels.\nOur approach introduces a context-aware labeling mechanism that dynamically adapts background representations based on the surrounding anatomical structures. The framework consists of two key components: (1) a context encoder that captures spatial relationships between foreground and background regions, and (2) a label refinement module that generates context-specific background labels.\nWe evaluate our method on multiple medical image segmentation datasets, including brain tumor segmentation and multi-organ segmentation. Experimental results demonstrate that CLL significantly improves segmentation accuracy, particularly for challenging cases where background context is crucial for accurate delineation.\nThe proposed framework provides a more principled approach to handling background classes in medical image segmentation, leading to improved performance and better clinical interpretability."
  },
  {
    "objectID": "publication/2019-12-01-deepvolume-brain-mri/index.html",
    "href": "publication/2019-12-01-deepvolume-brain-mri/index.html",
    "title": "DeepVolume: Brain Structure and Spatial Connection-Aware Network for Brain MRI Super-Resolution",
    "section": "",
    "text": "Magnetic Resonance Imaging (MRI) super-resolution is crucial for improving the quality of brain imaging data, particularly when high-resolution scans are unavailable or costly to acquire. Traditional super-resolution methods often fail to preserve the complex structural relationships and spatial connections inherent in brain anatomy. In this work, we propose DeepVolume, a novel deep learning framework that incorporates brain structure and spatial connection awareness for brain MRI super-resolution.\nOur approach leverages anatomical knowledge by integrating brain structure priors and spatial connectivity information into the super-resolution process. The framework consists of three key components: (1) a structure-aware encoder that captures brain anatomical features, (2) a spatial connection module that preserves inter-regional relationships, and (3) a high-resolution decoder that generates detailed brain images while maintaining structural integrity.\nWe evaluate DeepVolume on multiple brain MRI datasets, including T1-weighted, T2-weighted, and FLAIR sequences. Experimental results demonstrate that our method significantly outperforms existing super-resolution approaches in terms of both quantitative metrics and visual quality. The generated high-resolution images preserve fine anatomical details and maintain the spatial relationships between brain structures.\nDeepVolume represents a significant advancement in brain MRI super-resolution, providing a more anatomically accurate and clinically relevant approach to improving image resolution while preserving the complex structural characteristics of brain tissue."
  },
  {
    "objectID": "publication/2019-12-01-deepvolume-brain-mri/index.html#abstract",
    "href": "publication/2019-12-01-deepvolume-brain-mri/index.html#abstract",
    "title": "DeepVolume: Brain Structure and Spatial Connection-Aware Network for Brain MRI Super-Resolution",
    "section": "",
    "text": "Magnetic Resonance Imaging (MRI) super-resolution is crucial for improving the quality of brain imaging data, particularly when high-resolution scans are unavailable or costly to acquire. Traditional super-resolution methods often fail to preserve the complex structural relationships and spatial connections inherent in brain anatomy. In this work, we propose DeepVolume, a novel deep learning framework that incorporates brain structure and spatial connection awareness for brain MRI super-resolution.\nOur approach leverages anatomical knowledge by integrating brain structure priors and spatial connectivity information into the super-resolution process. The framework consists of three key components: (1) a structure-aware encoder that captures brain anatomical features, (2) a spatial connection module that preserves inter-regional relationships, and (3) a high-resolution decoder that generates detailed brain images while maintaining structural integrity.\nWe evaluate DeepVolume on multiple brain MRI datasets, including T1-weighted, T2-weighted, and FLAIR sequences. Experimental results demonstrate that our method significantly outperforms existing super-resolution approaches in terms of both quantitative metrics and visual quality. The generated high-resolution images preserve fine anatomical details and maintain the spatial relationships between brain structures.\nDeepVolume represents a significant advancement in brain MRI super-resolution, providing a more anatomically accurate and clinically relevant approach to improving image resolution while preserving the complex structural characteristics of brain tissue."
  },
  {
    "objectID": "publication/2022-03-01-learn2reg-comprehensive-challenge/index.html",
    "href": "publication/2022-03-01-learn2reg-comprehensive-challenge/index.html",
    "title": "Learn2Reg: Comprehensive Multi-Task Medical Image Registration Challenge, Dataset and Evaluation in the Era of Deep Learning",
    "section": "",
    "text": "Medical image registration is a fundamental task in medical image analysis, with applications ranging from image-guided surgery to longitudinal studies. However, the field lacks standardized benchmarks and evaluation frameworks, making it difficult to compare different approaches. In this work, we present Learn2Reg, a comprehensive multi-task medical image registration challenge that provides standardized datasets and evaluation metrics.\nOur challenge encompasses multiple registration tasks including inter-patient, intra-patient, and multi-modal registration scenarios. The framework consists of three main components: (1) a diverse dataset collection covering various anatomical regions and imaging modalities, (2) standardized evaluation metrics that assess both accuracy and robustness, and (3) a comprehensive leaderboard system that enables fair comparison of different approaches.\nWe evaluate the challenge on a large-scale dataset involving multiple institutions and imaging protocols. Results demonstrate the effectiveness of deep learning approaches in medical image registration while highlighting the importance of standardized evaluation frameworks. The challenge provides valuable insights into the current state of the field and identifies areas for future improvement.\nLearn2Reg represents a significant contribution to the medical image registration community, providing a standardized platform for method comparison and advancement of the field."
  },
  {
    "objectID": "publication/2022-03-01-learn2reg-comprehensive-challenge/index.html#abstract",
    "href": "publication/2022-03-01-learn2reg-comprehensive-challenge/index.html#abstract",
    "title": "Learn2Reg: Comprehensive Multi-Task Medical Image Registration Challenge, Dataset and Evaluation in the Era of Deep Learning",
    "section": "",
    "text": "Medical image registration is a fundamental task in medical image analysis, with applications ranging from image-guided surgery to longitudinal studies. However, the field lacks standardized benchmarks and evaluation frameworks, making it difficult to compare different approaches. In this work, we present Learn2Reg, a comprehensive multi-task medical image registration challenge that provides standardized datasets and evaluation metrics.\nOur challenge encompasses multiple registration tasks including inter-patient, intra-patient, and multi-modal registration scenarios. The framework consists of three main components: (1) a diverse dataset collection covering various anatomical regions and imaging modalities, (2) standardized evaluation metrics that assess both accuracy and robustness, and (3) a comprehensive leaderboard system that enables fair comparison of different approaches.\nWe evaluate the challenge on a large-scale dataset involving multiple institutions and imaging protocols. Results demonstrate the effectiveness of deep learning approaches in medical image registration while highlighting the importance of standardized evaluation frameworks. The challenge provides valuable insights into the current state of the field and identifies areas for future improvement.\nLearn2Reg represents a significant contribution to the medical image registration community, providing a standardized platform for method comparison and advancement of the field."
  },
  {
    "objectID": "publication/2024-07-01-fedfdd-federated-frequency-domain-decomposition/index.html",
    "href": "publication/2024-07-01-fedfdd-federated-frequency-domain-decomposition/index.html",
    "title": "FedFDD: Federated Learning with Frequency Domain Decomposition for Low-Dose CT Denoising",
    "section": "",
    "text": "Low-dose CT denoising is crucial for reducing radiation exposure while maintaining diagnostic quality. However, developing robust denoising models often requires large datasets that are distributed across multiple institutions, raising privacy concerns. In this work, we propose FedFDD, a federated learning approach with frequency domain decomposition for low-dose CT denoising.\nOur method introduces a frequency domain decomposition framework that enables effective federated learning while preserving data privacy. The framework consists of three key components: (1) a frequency domain decomposition module that separates low and high-frequency components, (2) a federated learning protocol that enables collaborative training without sharing raw data, and (3) a denoising module that reconstructs high-quality CT images.\nWe evaluate our approach on multiple low-dose CT datasets from different institutions. Experimental results demonstrate that our frequency domain approach significantly improves denoising performance in federated settings compared to traditional methods. The method shows robust performance across different noise levels and imaging protocols while maintaining strict privacy standards.\nThe proposed framework represents a significant advancement in privacy-preserving medical image processing, providing effective denoising that could improve diagnostic quality while reducing radiation exposure."
  },
  {
    "objectID": "publication/2024-07-01-fedfdd-federated-frequency-domain-decomposition/index.html#abstract",
    "href": "publication/2024-07-01-fedfdd-federated-frequency-domain-decomposition/index.html#abstract",
    "title": "FedFDD: Federated Learning with Frequency Domain Decomposition for Low-Dose CT Denoising",
    "section": "",
    "text": "Low-dose CT denoising is crucial for reducing radiation exposure while maintaining diagnostic quality. However, developing robust denoising models often requires large datasets that are distributed across multiple institutions, raising privacy concerns. In this work, we propose FedFDD, a federated learning approach with frequency domain decomposition for low-dose CT denoising.\nOur method introduces a frequency domain decomposition framework that enables effective federated learning while preserving data privacy. The framework consists of three key components: (1) a frequency domain decomposition module that separates low and high-frequency components, (2) a federated learning protocol that enables collaborative training without sharing raw data, and (3) a denoising module that reconstructs high-quality CT images.\nWe evaluate our approach on multiple low-dose CT datasets from different institutions. Experimental results demonstrate that our frequency domain approach significantly improves denoising performance in federated settings compared to traditional methods. The method shows robust performance across different noise levels and imaging protocols while maintaining strict privacy standards.\nThe proposed framework represents a significant advancement in privacy-preserving medical image processing, providing effective denoising that could improve diagnostic quality while reducing radiation exposure."
  },
  {
    "objectID": "publication/2022-09-01-maxstyle-adversarial-style-composition/index.html",
    "href": "publication/2022-09-01-maxstyle-adversarial-style-composition/index.html",
    "title": "MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation",
    "section": "",
    "text": "Medical image segmentation often faces challenges with domain shifts and style variations that can significantly impact model performance. Traditional approaches may struggle to maintain robustness across different imaging styles and protocols. In this work, we propose MaxStyle, an adversarial style composition approach for robust medical image segmentation.\nOur method introduces an adversarial training framework that generates diverse style variations to improve model robustness. The framework consists of three key components: (1) a style composition module that creates diverse imaging styles, (2) an adversarial training mechanism that challenges the segmentation model, and (3) a robust segmentation network that learns to handle style variations.\nWe evaluate our approach on multiple medical imaging datasets with style variations, including cross-scanner and cross-institution scenarios. Experimental results demonstrate that our MaxStyle approach significantly improves segmentation robustness compared to traditional methods. The method shows excellent performance in handling style variations while maintaining segmentation accuracy.\nThe proposed framework represents a significant advancement in robust medical image segmentation, providing more reliable results that could improve clinical applications across different imaging protocols."
  },
  {
    "objectID": "publication/2022-09-01-maxstyle-adversarial-style-composition/index.html#abstract",
    "href": "publication/2022-09-01-maxstyle-adversarial-style-composition/index.html#abstract",
    "title": "MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation",
    "section": "",
    "text": "Medical image segmentation often faces challenges with domain shifts and style variations that can significantly impact model performance. Traditional approaches may struggle to maintain robustness across different imaging styles and protocols. In this work, we propose MaxStyle, an adversarial style composition approach for robust medical image segmentation.\nOur method introduces an adversarial training framework that generates diverse style variations to improve model robustness. The framework consists of three key components: (1) a style composition module that creates diverse imaging styles, (2) an adversarial training mechanism that challenges the segmentation model, and (3) a robust segmentation network that learns to handle style variations.\nWe evaluate our approach on multiple medical imaging datasets with style variations, including cross-scanner and cross-institution scenarios. Experimental results demonstrate that our MaxStyle approach significantly improves segmentation robustness compared to traditional methods. The method shows excellent performance in handling style variations while maintaining segmentation accuracy.\nThe proposed framework represents a significant advancement in robust medical image segmentation, providing more reliable results that could improve clinical applications across different imaging protocols."
  },
  {
    "objectID": "publication/2019-10-01-overfitting-neural-nets-class-imbalance/index.html",
    "href": "publication/2019-10-01-overfitting-neural-nets-class-imbalance/index.html",
    "title": "Overfitting of Neural Nets Under Class Imbalance: Analysis and Improvements for Segmentation",
    "section": "",
    "text": "Class imbalance is a pervasive challenge in medical image segmentation, where certain anatomical structures or pathological regions may be significantly underrepresented in the training data. This imbalance often leads to overfitting, where neural networks memorize the training data rather than learning generalizable features. In this work, we present a comprehensive analysis of overfitting behavior under class imbalance conditions and propose effective improvements.\nOur study investigates the relationship between class imbalance ratios and overfitting patterns across different network architectures and training strategies. We identify key factors that contribute to overfitting, including the degree of class imbalance, network capacity, and regularization techniques. Through extensive experiments on multiple medical imaging datasets, we demonstrate that overfitting manifests differently across classes, with minority classes being particularly susceptible.\nWe propose several strategies to mitigate overfitting under class imbalance, including adaptive learning rates, class-specific regularization, and curriculum learning approaches. Our analysis provides insights into the fundamental mechanisms driving overfitting in imbalanced medical image segmentation tasks and offers practical guidelines for model development.\nThe findings from this work contribute to a better understanding of neural network behavior under class imbalance conditions and provide a foundation for developing more robust segmentation models in medical imaging applications."
  },
  {
    "objectID": "publication/2019-10-01-overfitting-neural-nets-class-imbalance/index.html#abstract",
    "href": "publication/2019-10-01-overfitting-neural-nets-class-imbalance/index.html#abstract",
    "title": "Overfitting of Neural Nets Under Class Imbalance: Analysis and Improvements for Segmentation",
    "section": "",
    "text": "Class imbalance is a pervasive challenge in medical image segmentation, where certain anatomical structures or pathological regions may be significantly underrepresented in the training data. This imbalance often leads to overfitting, where neural networks memorize the training data rather than learning generalizable features. In this work, we present a comprehensive analysis of overfitting behavior under class imbalance conditions and propose effective improvements.\nOur study investigates the relationship between class imbalance ratios and overfitting patterns across different network architectures and training strategies. We identify key factors that contribute to overfitting, including the degree of class imbalance, network capacity, and regularization techniques. Through extensive experiments on multiple medical imaging datasets, we demonstrate that overfitting manifests differently across classes, with minority classes being particularly susceptible.\nWe propose several strategies to mitigate overfitting under class imbalance, including adaptive learning rates, class-specific regularization, and curriculum learning approaches. Our analysis provides insights into the fundamental mechanisms driving overfitting in imbalanced medical image segmentation tasks and offers practical guidelines for model development.\nThe findings from this work contribute to a better understanding of neural network behavior under class imbalance conditions and provide a foundation for developing more robust segmentation models in medical imaging applications."
  },
  {
    "objectID": "publication/2017-12-01-noninvasive-idh1-mutation-estimation/index.html",
    "href": "publication/2017-12-01-noninvasive-idh1-mutation-estimation/index.html",
    "title": "Noninvasive IDH1 Mutation Estimation Based on a Quantitative Radiomics Approach for Grade II Glioma",
    "section": "",
    "text": "Isocitrate dehydrogenase 1 (IDH1) mutation status is a critical prognostic factor in grade II gliomas, traditionally requiring invasive biopsy for determination. Noninvasive prediction of IDH1 status could significantly improve patient management and treatment planning. In this work, we propose a quantitative radiomics approach for noninvasive estimation of IDH1 mutation status in grade II gliomas.\nOur method leverages quantitative imaging features extracted from conventional MRI sequences to predict IDH1 mutation status without requiring invasive procedures. The framework consists of three key components: (1) a comprehensive radiomics feature extraction module that captures quantitative imaging biomarkers, (2) a feature selection mechanism that identifies the most predictive features, and (3) a prediction model that provides noninvasive IDH1 estimation.\nWe evaluate our approach on a large cohort of grade II glioma patients with confirmed IDH1 mutation status. Experimental results demonstrate that our quantitative radiomics approach significantly improves prediction accuracy compared to traditional methods. The method shows robust performance across different MRI protocols and provides valuable insights into the relationship between imaging features and molecular characteristics.\nThe proposed framework represents a significant advancement in noninvasive glioma characterization, providing more accurate IDH1 prediction that could reduce the need for invasive procedures and improve patient outcomes."
  },
  {
    "objectID": "publication/2017-12-01-noninvasive-idh1-mutation-estimation/index.html#abstract",
    "href": "publication/2017-12-01-noninvasive-idh1-mutation-estimation/index.html#abstract",
    "title": "Noninvasive IDH1 Mutation Estimation Based on a Quantitative Radiomics Approach for Grade II Glioma",
    "section": "",
    "text": "Isocitrate dehydrogenase 1 (IDH1) mutation status is a critical prognostic factor in grade II gliomas, traditionally requiring invasive biopsy for determination. Noninvasive prediction of IDH1 status could significantly improve patient management and treatment planning. In this work, we propose a quantitative radiomics approach for noninvasive estimation of IDH1 mutation status in grade II gliomas.\nOur method leverages quantitative imaging features extracted from conventional MRI sequences to predict IDH1 mutation status without requiring invasive procedures. The framework consists of three key components: (1) a comprehensive radiomics feature extraction module that captures quantitative imaging biomarkers, (2) a feature selection mechanism that identifies the most predictive features, and (3) a prediction model that provides noninvasive IDH1 estimation.\nWe evaluate our approach on a large cohort of grade II glioma patients with confirmed IDH1 mutation status. Experimental results demonstrate that our quantitative radiomics approach significantly improves prediction accuracy compared to traditional methods. The method shows robust performance across different MRI protocols and provides valuable insights into the relationship between imaging features and molecular characteristics.\nThe proposed framework represents a significant advancement in noninvasive glioma characterization, providing more accurate IDH1 prediction that could reduce the need for invasive procedures and improve patient outcomes."
  },
  {
    "objectID": "publication/2022-06-01-enhancing-mr-segmentation-adversarial/index.html",
    "href": "publication/2022-06-01-enhancing-mr-segmentation-adversarial/index.html",
    "title": "Enhancing MR Image Segmentation with Realistic Adversarial Data Augmentation",
    "section": "",
    "text": "Data augmentation is crucial for improving the robustness and generalization of deep learning models in medical image segmentation. However, traditional augmentation techniques often fail to capture the complex variations present in real medical images. In this work, we propose a novel approach for enhancing MR image segmentation through realistic adversarial data augmentation.\nOur method introduces an adversarial training framework that generates realistic augmented samples by learning the underlying data distribution of medical images. The approach consists of two key components: (1) a realistic data generator that produces augmented samples that maintain anatomical plausibility, and (2) a segmentation network that learns robust features from both original and augmented data.\nWe evaluate our approach on multiple MR image segmentation tasks, including brain tumor segmentation and cardiac structure segmentation. Experimental results demonstrate that our realistic adversarial augmentation significantly improves segmentation performance compared to traditional augmentation methods. The generated samples maintain anatomical consistency while introducing meaningful variations that enhance model robustness.\nThe proposed framework represents a significant advancement in medical image augmentation, providing more effective training data that leads to improved segmentation performance and better generalization to unseen data."
  },
  {
    "objectID": "publication/2022-06-01-enhancing-mr-segmentation-adversarial/index.html#abstract",
    "href": "publication/2022-06-01-enhancing-mr-segmentation-adversarial/index.html#abstract",
    "title": "Enhancing MR Image Segmentation with Realistic Adversarial Data Augmentation",
    "section": "",
    "text": "Data augmentation is crucial for improving the robustness and generalization of deep learning models in medical image segmentation. However, traditional augmentation techniques often fail to capture the complex variations present in real medical images. In this work, we propose a novel approach for enhancing MR image segmentation through realistic adversarial data augmentation.\nOur method introduces an adversarial training framework that generates realistic augmented samples by learning the underlying data distribution of medical images. The approach consists of two key components: (1) a realistic data generator that produces augmented samples that maintain anatomical plausibility, and (2) a segmentation network that learns robust features from both original and augmented data.\nWe evaluate our approach on multiple MR image segmentation tasks, including brain tumor segmentation and cardiac structure segmentation. Experimental results demonstrate that our realistic adversarial augmentation significantly improves segmentation performance compared to traditional augmentation methods. The generated samples maintain anatomical consistency while introducing meaningful variations that enhance model robustness.\nThe proposed framework represents a significant advancement in medical image augmentation, providing more effective training data that leads to improved segmentation performance and better generalization to unseen data."
  },
  {
    "objectID": "publication/2021-04-01-novel-image-signature-radiomics/index.html",
    "href": "publication/2021-04-01-novel-image-signature-radiomics/index.html",
    "title": "A Novel Image Signature-Based Radiomics Method to Achieve Precise Diagnosis and Prognostic Stratification of Gliomas",
    "section": "",
    "text": "Glioma diagnosis and prognosis prediction remain challenging due to the heterogeneity of these tumors and the limitations of current diagnostic approaches. Traditional methods often rely on subjective visual assessment, which can lead to inconsistent results. In this work, we propose a novel image signature-based radiomics method for precise glioma diagnosis and prognostic stratification.\nOur approach introduces a comprehensive radiomics framework that extracts quantitative imaging features from multiple MRI sequences. The method consists of three key components: (1) an image signature extraction module that identifies distinctive imaging patterns, (2) a feature selection mechanism that identifies the most predictive biomarkers, and (3) a prognostic stratification model that provides personalized risk assessment.\nWe evaluate our method on a large cohort of glioma patients with comprehensive clinical follow-up data. Experimental results demonstrate that our image signature-based approach significantly improves diagnostic accuracy and prognostic prediction compared to traditional methods. The method shows robust performance across different glioma subtypes and provides valuable insights into tumor biology.\nThe proposed framework represents a significant advancement in glioma characterization, providing more precise diagnosis and personalized prognostic information that could improve patient management and treatment planning."
  },
  {
    "objectID": "publication/2021-04-01-novel-image-signature-radiomics/index.html#abstract",
    "href": "publication/2021-04-01-novel-image-signature-radiomics/index.html#abstract",
    "title": "A Novel Image Signature-Based Radiomics Method to Achieve Precise Diagnosis and Prognostic Stratification of Gliomas",
    "section": "",
    "text": "Glioma diagnosis and prognosis prediction remain challenging due to the heterogeneity of these tumors and the limitations of current diagnostic approaches. Traditional methods often rely on subjective visual assessment, which can lead to inconsistent results. In this work, we propose a novel image signature-based radiomics method for precise glioma diagnosis and prognostic stratification.\nOur approach introduces a comprehensive radiomics framework that extracts quantitative imaging features from multiple MRI sequences. The method consists of three key components: (1) an image signature extraction module that identifies distinctive imaging patterns, (2) a feature selection mechanism that identifies the most predictive biomarkers, and (3) a prognostic stratification model that provides personalized risk assessment.\nWe evaluate our method on a large cohort of glioma patients with comprehensive clinical follow-up data. Experimental results demonstrate that our image signature-based approach significantly improves diagnostic accuracy and prognostic prediction compared to traditional methods. The method shows robust performance across different glioma subtypes and provides valuable insights into tumor biology.\nThe proposed framework represents a significant advancement in glioma characterization, providing more precise diagnosis and personalized prognostic information that could improve patient management and treatment planning."
  },
  {
    "objectID": "publication/2019-01-01-super-resolution-reconstruction-ultrasound/index.html",
    "href": "publication/2019-01-01-super-resolution-reconstruction-ultrasound/index.html",
    "title": "Super-Resolution Reconstruction of Plane-Wave Ultrasound Image Based on a Multi-Angle Parallel U-Net with Maxout Unit and Novel Loss Function",
    "section": "",
    "text": "Plane-wave ultrasound imaging offers advantages in terms of frame rate and penetration depth, but often suffers from reduced image quality compared to focused ultrasound. Super-resolution reconstruction techniques can enhance image quality, but traditional approaches often fail to preserve fine details and anatomical structures. In this work, we propose a novel multi-angle parallel U-Net architecture with maxout units for ultrasound image super-resolution.\nOur approach introduces a parallel processing framework that handles multiple imaging angles simultaneously. The framework consists of three key components: (1) a multi-angle parallel U-Net that processes different viewing angles, (2) maxout units that enhance feature representation, and (3) a novel loss function that preserves both structural and textural details.\nWe evaluate our method on plane-wave ultrasound datasets with ground truth high-resolution images. Experimental results demonstrate that our approach significantly improves image quality compared to existing super-resolution methods. The method shows excellent performance in preserving fine anatomical details and reducing artifacts commonly found in ultrasound imaging.\nThe proposed framework represents a significant advancement in ultrasound image processing, providing higher quality images that could improve diagnostic accuracy in clinical applications."
  },
  {
    "objectID": "publication/2019-01-01-super-resolution-reconstruction-ultrasound/index.html#abstract",
    "href": "publication/2019-01-01-super-resolution-reconstruction-ultrasound/index.html#abstract",
    "title": "Super-Resolution Reconstruction of Plane-Wave Ultrasound Image Based on a Multi-Angle Parallel U-Net with Maxout Unit and Novel Loss Function",
    "section": "",
    "text": "Plane-wave ultrasound imaging offers advantages in terms of frame rate and penetration depth, but often suffers from reduced image quality compared to focused ultrasound. Super-resolution reconstruction techniques can enhance image quality, but traditional approaches often fail to preserve fine details and anatomical structures. In this work, we propose a novel multi-angle parallel U-Net architecture with maxout units for ultrasound image super-resolution.\nOur approach introduces a parallel processing framework that handles multiple imaging angles simultaneously. The framework consists of three key components: (1) a multi-angle parallel U-Net that processes different viewing angles, (2) maxout units that enhance feature representation, and (3) a novel loss function that preserves both structural and textural details.\nWe evaluate our method on plane-wave ultrasound datasets with ground truth high-resolution images. Experimental results demonstrate that our approach significantly improves image quality compared to existing super-resolution methods. The method shows excellent performance in preserving fine anatomical details and reducing artifacts commonly found in ultrasound imaging.\nThe proposed framework represents a significant advancement in ultrasound image processing, providing higher quality images that could improve diagnostic accuracy in clinical applications."
  },
  {
    "objectID": "publication/2023-06-27-joint-optimization-segmentation/index.html",
    "href": "publication/2023-06-27-joint-optimization-segmentation/index.html",
    "title": "Joint Optimization of Class-Specific Training- and Test-Time Data Augmentation in Segmentation",
    "section": "",
    "text": "Data augmentation is a crucial technique for improving the performance and robustness of deep learning models in medical image segmentation. Traditional approaches apply the same augmentation strategy to all classes, which may not be optimal since different anatomical structures have varying characteristics and challenges. In this work, we propose a novel framework for joint optimization of class-specific training- and test-time data augmentation in segmentation tasks.\nOur approach introduces class-specific augmentation policies that are learned during training and can be adapted at test time. The framework consists of two main components: (1) a class-specific augmentation module that learns optimal augmentation strategies for each class, and (2) a test-time adaptation mechanism that fine-tunes these strategies based on the input data.\nWe evaluate our method on multiple medical image segmentation datasets, including brain tumor segmentation and multi-organ segmentation. Experimental results demonstrate that our class-specific augmentation approach significantly outperforms standard augmentation techniques, achieving improved segmentation accuracy and robustness across different anatomical structures.\nThe proposed framework provides a more principled approach to data augmentation in medical image segmentation, offering better performance while maintaining interpretability and clinical relevance."
  },
  {
    "objectID": "publication/2023-06-27-joint-optimization-segmentation/index.html#abstract",
    "href": "publication/2023-06-27-joint-optimization-segmentation/index.html#abstract",
    "title": "Joint Optimization of Class-Specific Training- and Test-Time Data Augmentation in Segmentation",
    "section": "",
    "text": "Data augmentation is a crucial technique for improving the performance and robustness of deep learning models in medical image segmentation. Traditional approaches apply the same augmentation strategy to all classes, which may not be optimal since different anatomical structures have varying characteristics and challenges. In this work, we propose a novel framework for joint optimization of class-specific training- and test-time data augmentation in segmentation tasks.\nOur approach introduces class-specific augmentation policies that are learned during training and can be adapted at test time. The framework consists of two main components: (1) a class-specific augmentation module that learns optimal augmentation strategies for each class, and (2) a test-time adaptation mechanism that fine-tunes these strategies based on the input data.\nWe evaluate our method on multiple medical image segmentation datasets, including brain tumor segmentation and multi-organ segmentation. Experimental results demonstrate that our class-specific augmentation approach significantly outperforms standard augmentation techniques, achieving improved segmentation accuracy and robustness across different anatomical structures.\nThe proposed framework provides a more principled approach to data augmentation in medical image segmentation, offering better performance while maintaining interpretability and clinical relevance."
  },
  {
    "objectID": "publication/2022-09-01-fetal-cortex-segmentation-topology-thickness/index.html",
    "href": "publication/2022-09-01-fetal-cortex-segmentation-topology-thickness/index.html",
    "title": "Fetal Cortex Segmentation with Topology and Thickness Loss Constraints",
    "section": "",
    "text": "Fetal cortex segmentation is crucial for understanding brain development and detecting developmental abnormalities. However, the complex and rapidly developing nature of the fetal brain presents unique challenges for segmentation. In this work, we propose a novel approach for fetal cortex segmentation that incorporates topology and thickness loss constraints.\nOur method introduces anatomical constraints that ensure the segmented cortex maintains proper topological structure and realistic thickness measurements. The framework consists of three key components: (1) a topology-aware segmentation module that preserves cortical structure, (2) a thickness estimation module that measures cortical thickness, and (3) a combined loss function that optimizes both topology and thickness constraints.\nWe evaluate our approach on fetal brain MRI datasets with expert annotations. Experimental results demonstrate that our topology and thickness-constrained approach significantly improves segmentation accuracy compared to traditional methods. The method shows excellent performance in maintaining anatomical consistency and providing reliable thickness measurements.\nThe proposed framework represents a significant advancement in fetal brain analysis, providing more accurate segmentation that could improve our understanding of brain development and early detection of developmental disorders."
  },
  {
    "objectID": "publication/2022-09-01-fetal-cortex-segmentation-topology-thickness/index.html#abstract",
    "href": "publication/2022-09-01-fetal-cortex-segmentation-topology-thickness/index.html#abstract",
    "title": "Fetal Cortex Segmentation with Topology and Thickness Loss Constraints",
    "section": "",
    "text": "Fetal cortex segmentation is crucial for understanding brain development and detecting developmental abnormalities. However, the complex and rapidly developing nature of the fetal brain presents unique challenges for segmentation. In this work, we propose a novel approach for fetal cortex segmentation that incorporates topology and thickness loss constraints.\nOur method introduces anatomical constraints that ensure the segmented cortex maintains proper topological structure and realistic thickness measurements. The framework consists of three key components: (1) a topology-aware segmentation module that preserves cortical structure, (2) a thickness estimation module that measures cortical thickness, and (3) a combined loss function that optimizes both topology and thickness constraints.\nWe evaluate our approach on fetal brain MRI datasets with expert annotations. Experimental results demonstrate that our topology and thickness-constrained approach significantly improves segmentation accuracy compared to traditional methods. The method shows excellent performance in maintaining anatomical consistency and providing reliable thickness measurements.\nThe proposed framework represents a significant advancement in fetal brain analysis, providing more accurate segmentation that could improve our understanding of brain development and early detection of developmental disorders."
  },
  {
    "objectID": "hiring.html#about-me",
    "href": "hiring.html#about-me",
    "title": "Research Vacancies at Fudan University",
    "section": "About Me",
    "text": "About Me\nÊùéÊ≥ΩÊ¶âÔºåÁé∞Â§çÊó¶Â§ßÂ≠¶ÁîüÁâ©ÂåªÂ≠¶Â∑•Á®ã‰∏éÊäÄÊúØÂàõÊñ∞Â≠¶Èô¢ÈùíÂπ¥Á†îÁ©∂ÂëòÔºåËé∑ÂõΩÂÆ∂Ëá™ÁÑ∂ÁßëÂ≠¶Âü∫ÈáëÂßîÔºàÊµ∑Â§ñÔºâ‰ºòÈùíÈ°πÁõÆËµÑÂä©Ôºå‰ª•Âèä‰∏äÊµ∑Â∏ÇÈ´òÂ±ÇÊ¨°ÈùíÂπ¥‰∫∫ÊâçËÆ°ÂàíÔºàÊµ¶Ê±üÈ°πÁõÆÔºâËµÑÂä©„ÄÇ ‰ªñ‰∫é2023Âπ¥1ÊúàÂú®Â∏ùÂõΩÁêÜÂ∑•Â≠¶Èô¢ComputingÁ≥ªËé∑ÂæóÂçöÂ£´Â≠¶‰ΩçÔºåÊ≠§ÂâçÊòØÁâõÊ¥•Â§ßÂ≠¶FMRIB CentreÁöÑÂçöÂ£´ÂêéÁ†îÁ©∂Âëò„ÄÇ\n\nÁ†îÁ©∂ÊñπÂêëÈõÜ‰∏≠Âú®Êú∫Âô®Â≠¶‰π†ÂèäÂÖ∂Âú®ÂåªÂ≠¶ÂõæÂÉèÂ§ÑÁêÜ‰∏≠ÁöÑÂ∫îÁî®Ôºå‰æßÈáçÁ•ûÁªèÂΩ±ÂÉèÂàÜÊûêÂíåËß£ÂÜ≥‰∏¥Â∫äÈóÆÈ¢ò„ÄÇ\nÂèëË°®Â≠¶ÊúØËÆ∫Êñá30‰ΩôÁØáÔºåÂÖ∂‰∏≠Â§öÁØáÂèëË°®‰∫éTMIÂíåMICCAIÔºõGoogle ScholarÂºïÁî®Ë∂ÖËøá4000Ê¨°„ÄÇ\nÊõæËé∑ÂæóÂçé‰∏∫AIÊåëÊàòËµõÂíåMICCAIÊåëÊàòËµõÂÜ†ÂÜõÔºåÂπ∂ÊãÖ‰ªªMICCAI/MIDLÁ≠âÈ¢ÜÂüü‰∏ªË¶Å‰ºöËÆÆÁöÑÈ¢ÜÂüü‰∏ªÂ∏≠„ÄÇ\nËØ¶ÊÉÖËØ∑ËÆøÈóÆ‰∏™‰∫∫‰∏ªÈ°µÔºöhttps://zerojumpline.github.io„ÄÇ"
  },
  {
    "objectID": "hiring.html#who-were-looking-for",
    "href": "hiring.html#who-were-looking-for",
    "title": "Research Vacancies at Fudan University",
    "section": "Who We‚Äôre Looking For",
    "text": "Who We‚Äôre Looking For\nÊØèÂπ¥ÊãõÂãü1ÂêçÁ°ïÂ£´Á†îÁ©∂ÁîüÔºå2ÂêçÂçöÂ£´Á†îÁ©∂Áîü‰ª•ÂèäÂ§öÂêçÂçöÂ£´ÂêéÂíåÁßëÁ†îÂÆû‰π†ÁîüÔºåÊ¨¢ËøéÂûÇËØ¢„ÄÇÂ§çÊó¶Á°ïÂ£´ÂíåÁõ¥ÂçöÁîüÁöÑÊúÄ‰Ω≥ÂÖ•Â≠¶Êú∫‰ºöÂú®7Êúà‰ªΩÁöÑÂ§è‰ª§Ëê•ÔºåÊä•ÂêçÈúÄ‰ªé5ÊúàÂºÄÂßãÂáÜÂ§á„ÄÇÊôÆÂçöÁîüÂíåÂçöÂ£´ÂêéÂÖ®Âπ¥ÂùáÂèØÁî≥ËØ∑ÔºåËØ∑ÂèäÊó∂ËÅîÁ≥ª„ÄÇ\n\nÁ†îÁ©∂ÊñπÂêëÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºö\n\nSafe and robust AI approaches.\nAdvanced medical imaging algorithms.\nAI for Healthcare.\nÂÖ∑‰ΩìÁ†îÁ©∂ÊñπÂêëÂèØÊ†πÊçÆÂ≠¶ÁîüÂÖ¥Ë∂£Á°ÆÂÆö„ÄÇ\n\n\n\nÂü∫Á°ÄË¶ÅÊ±ÇÔºö\n\nËÆ°ÁÆóÊú∫„ÄÅÁîµÂ≠êÂ∑•Á®ãÊàñÁîüÁâ©ÂåªÂ≠¶Â∑•Á®ãÁ≠âÁõ∏ÂÖ≥È¢ÜÂüüÊïôËÇ≤ËÉåÊôØ„ÄÇ\nÁÜüÁªÉÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂ÔºàÂ¶ÇPyTorchÔºâÂèäËΩØ‰ª∂ÂºÄÂèëÂ∑•ÂÖ∑ÔºàÂ¶ÇGitÔºâ„ÄÇ\nÂØπAIÂú®ÂåªÁñóÂÅ•Â∫∑ÊàñÁ•ûÁªèÁßëÂ≠¶‰∏≠ÁöÑÂ∫îÁî®ÊúâÊµìÂéöÂÖ¥Ë∂£„ÄÇ\nÂÖ∑Â§áÂõ¢ÈòüÂêà‰ΩúÁ≤æÁ•ûÔºåËÉΩÂ§üÂú®Ë∑®Â≠¶ÁßëÂõ¢Èòü‰∏≠Â≠¶‰π†ÊàêÈïø„ÄÇ\n\n\n\n‰ºòÂÖàËÄÉËôëÔºö\n\nÊúâ‰ª•‰∏ãÁ†îÁ©∂ÁªèÈ™å‰πã‰∏ÄÔºö\n\nGenerative model (e.g.¬†diffusion model, VAEs);\nInterpretebility methods (e.g.¬†causal model, explainable AI);\nLarge multimodal transformers (e.g.¬†LLMs, knowledge graph);\nMRI acquisition and postprocessing (e.g.¬†MRI reconstruction, segmentation).\n\nÊã•ÊúâÁõ¥Êé•ÂèÇ‰∏éÂåªÂ≠¶ÂõæÂÉèÊàñÂåªÂ≠¶‰ø°ÊÅØÂ§ÑÁêÜÂ∑•‰ΩúÁöÑÁªèÈ™å„ÄÇ\nÂú®ÂåªÂ≠¶ÂõæÂÉè„ÄÅÊú∫Âô®Â≠¶‰π†ÊàñÁªºÂêàÊóóËà∞‰ºöËÆÆ/ÊúüÂàä‰∏ä‰ª•Á¨¨‰∏Ä‰ΩúËÄÖË∫´‰ªΩÂèëË°®ËÆ∫Êñá„ÄÇ"
  },
  {
    "objectID": "hiring.html#why-join-us",
    "href": "hiring.html#why-join-us",
    "title": "Research Vacancies at Fudan University",
    "section": "Why Join Us?",
    "text": "Why Join Us?\n‰Ω†Â∞ÜÂä†ÂÖ•Áî±Â§çÊó¶Â§ßÂ≠¶ÂâØÊ†°ÈïøÊ±™Ê∫êÊ∫êÊïôÊéàÈ¢ÜÂØºÁöÑÂåªÂ≠¶‰ø°Âè∑Â§ÑÁêÜÂÆûÈ™åÂÆ§ÔºåÂõ¢ÈòüÁé∞ÊúâË∂ÖËøá30ÂêçÁ†îÁ©∂Áîü„ÄÇÂä†ÂÖ•Êàë‰ª¨Ôºå‰Ω†Â∞ÜËé∑ÂæóÔºö\n\nÂÖÖË∂≥ÁöÑËµÑÈáë„ÄÅËÆ°ÁÆóËµÑÊ∫êÂíå‰∏¥Â∫äËµÑÊ∫êÊîØÊåÅ„ÄÇ\nÂºÄÊîæÂåÖÂÆπÁöÑÂÆûÈ™åÂÆ§Ê∞õÂõ¥ÔºåÂØºÂ∏àÊ≥®ÈáçÂ≠¶Áîü‰∏™‰∫∫ÊàêÈïø„ÄÇ\n‰∏éÂõΩÂÜÖÂíåÂõΩÈôÖÈ°∂Á∫ßÁßëÁ†îÊú∫ÊûÑÁöÑËÆøÂ≠¶‰∫§ÊµÅÊú∫‰ºö„ÄÇ\n\nÊàë‰ª¨Êã•Êúâ‰∏éÊù•Ëá™‰∏çÂêåËÉåÊôØÁöÑÂêåÂ≠¶Âêà‰ΩúÁöÑÁªèÈ™åÔºåÂæóÁõä‰∫éÂ≠¶Ê†°ÁöÑÂπ≥Âè∞ÂíåÈ°πÁõÆÂÆûË∑µÔºåÂ§ßÂÆ∂ÁöÑÂêéÁª≠ÂèëÂ±ïÈÉΩÈùûÂ∏∏Â•ΩÔºà‰æãÂ¶ÇÔºåËé∑ÂæóOxford, Cambridge, ImperialÂíåTUMÁ≠âÈ´òÊ†°ÁöÑÂçöÂ£´Â≠¶‰ΩçÔºåÊàñËøõÂÖ•Google, Microsoft, MetaÂíåÂçé‰∏∫Á≠âÂÖ¨Âè∏Â∑•‰ΩúÔºâ„ÄÇ"
  },
  {
    "objectID": "hiring.html#how-to-apply",
    "href": "hiring.html#how-to-apply",
    "title": "Research Vacancies at Fudan University",
    "section": "How to Apply",
    "text": "How to Apply\nËØ∑Â∞Ü‰∏™‰∫∫ÁÆÄÂéÜÔºàÂåÖÂê´ÊïôËÇ≤ÂíåÁ†îÁ©∂ÁªèÂéÜÔºâ„ÄÅÁõ∏ÂÖ≥‰ø°ÊÅØÔºàÂ¶ÇGitHubË¥¶Âè∑„ÄÅÊàêÁª©Âçï„ÄÅËÆ∫ÊñáÔºâÂèëÈÄÅËá≥Ôºö zejuli@fudan.edu.cn„ÄÇ\nÈÇÆ‰ª∂Ê†áÈ¢òÊ†ºÂºè‰∏∫‚ÄúÂÖ•Â≠¶Êó∂Èó¥-È°πÁõÆ-‰Ω†ÁöÑÂßìÂêç‚ÄùÔºåÂ¶Ç‚Äù2025Áßã-ÂçöÂ£´-Âº†‰∏â‚Äù„ÄÇ\nËã•ÊÇ®ÁöÑËÉåÊôØÁ¨¶ÂêàËØ•ËÅå‰ΩçË¶ÅÊ±ÇÔºåÊàë‰ª¨Â∞ÜÂ∞ΩÂø´‰∏éÊÇ®ÂèñÂæóËÅîÁ≥ªÔºåÂπ∂ÂÆâÊéíÈù¢ËØï„ÄÇ\nÊàë‰ºöËÆ§ÁúüÈòÖËØªÊØè‰∏ÄÂ∞Å‰ª•Ê≠§Ê†áÈ¢òÊ†ºÂºèÂèëÈÄÅÁöÑÈÇÆ‰ª∂ÔºåÂπ∂Â∞ΩÈáèÂõûÂ§çÊØè‰∏Ä‰ΩçËÆ§ÁúüÂÜôÈÇÆ‰ª∂ÁöÑÂêåÂ≠¶„ÄÇ‰ΩÜÂØπ‰∫éÈÇ£‰∫õÁúã‰ººÁæ§ÂèëÁöÑÈÇÆ‰ª∂ÔºåÊàëÂèØËÉΩÊó†Ê≥ï‰∏Ä‰∏ÄÂõûÂ§ç„ÄÇ"
  },
  {
    "objectID": "hiring.html#a-few-more-words",
    "href": "hiring.html#a-few-more-words",
    "title": "Research Vacancies at Fudan University",
    "section": "A Few More Words",
    "text": "A Few More Words\nÊÑüË∞¢ÈòÖËØªÂà∞ËøôÈáåÁöÑÊØè‰∏Ä‰ΩçÂêåÂ≠¶„ÄÇÊàëËßâÂæóÂÅöÁ†îÁ©∂ÂíåÂàõÈÄ†Êñ∞‰∫ãÁâ©ÊòØÈùûÂ∏∏ÊúâÊÑèÊÄùÁöÑ‰∫ãÊÉÖÔºåÂπ∂‰∏îÂ§ßÈÉ®ÂàÜÊó∂Èó¥Êàë‰ºö‰∫´ÂèóËøô‰∏™ËøáÁ®ã„ÄÇÂ¶ÇÊûúÊúâÊú∫‰ºöÔºåÊàë‰πüÂ∏åÊúõ‰Ω†‰ª¨ËÉΩÂ§ü‰ΩìÈ™åÂà∞Á±ª‰ººÁöÑÊÑüËßâ„ÄÇ"
  },
  {
    "objectID": "publication/2023-09-01-robustness-stress-testing-medical-image/index.html",
    "href": "publication/2023-09-01-robustness-stress-testing-medical-image/index.html",
    "title": "Robustness Stress Testing in Medical Image Classification",
    "section": "",
    "text": "Robustness stress testing is crucial for ensuring the reliability of medical image classification models in clinical settings, where performance degradation can have serious consequences. Traditional evaluation approaches often focus on average performance metrics, failing to identify potential failure modes and vulnerabilities. In this work, we propose a comprehensive framework for robustness stress testing in medical image classification.\nOur method introduces systematic stress testing procedures that evaluate model performance under various challenging conditions. The framework consists of three key components: (1) a stress test generator that creates challenging scenarios, (2) a robustness evaluation module that assesses model performance under stress, and (3) a failure mode analysis system that identifies potential vulnerabilities.\nWe evaluate our approach on multiple medical image classification datasets with various stress conditions, including noise, artifacts, and domain shifts. Experimental results demonstrate that our stress testing framework effectively identifies model vulnerabilities and provides insights into failure modes. The method shows particular effectiveness in detecting robustness issues that may not be apparent through standard evaluation procedures.\nThe proposed framework represents a significant advancement in medical AI evaluation, providing more comprehensive assessment that could improve the safe deployment of medical image classification systems."
  },
  {
    "objectID": "publication/2023-09-01-robustness-stress-testing-medical-image/index.html#abstract",
    "href": "publication/2023-09-01-robustness-stress-testing-medical-image/index.html#abstract",
    "title": "Robustness Stress Testing in Medical Image Classification",
    "section": "",
    "text": "Robustness stress testing is crucial for ensuring the reliability of medical image classification models in clinical settings, where performance degradation can have serious consequences. Traditional evaluation approaches often focus on average performance metrics, failing to identify potential failure modes and vulnerabilities. In this work, we propose a comprehensive framework for robustness stress testing in medical image classification.\nOur method introduces systematic stress testing procedures that evaluate model performance under various challenging conditions. The framework consists of three key components: (1) a stress test generator that creates challenging scenarios, (2) a robustness evaluation module that assesses model performance under stress, and (3) a failure mode analysis system that identifies potential vulnerabilities.\nWe evaluate our approach on multiple medical image classification datasets with various stress conditions, including noise, artifacts, and domain shifts. Experimental results demonstrate that our stress testing framework effectively identifies model vulnerabilities and provides insights into failure modes. The method shows particular effectiveness in detecting robustness issues that may not be apparent through standard evaluation procedures.\nThe proposed framework represents a significant advancement in medical AI evaluation, providing more comprehensive assessment that could improve the safe deployment of medical image classification systems."
  },
  {
    "objectID": "publication/2017-09-01-age-groups-glioblastoma/index.html",
    "href": "publication/2017-09-01-age-groups-glioblastoma/index.html",
    "title": "Age Groups Related Glioblastoma Study Based on Radiomics Approach",
    "section": "",
    "text": "Glioblastoma exhibits different characteristics and outcomes across different age groups, making age-specific analysis crucial for personalized treatment strategies. Traditional approaches often treat glioblastoma as a homogeneous disease, failing to capture the age-related variations in tumor biology and patient outcomes. In this work, we present a comprehensive radiomics-based study investigating age-related differences in glioblastoma characteristics.\nOur approach leverages quantitative imaging features to characterize glioblastoma across different age groups, from young adults to elderly patients. The study analyzes multiple radiomics features including texture, shape, and intensity-based descriptors extracted from brain MRI scans. We investigate how these features vary across age groups and their correlation with clinical outcomes.\nWe evaluate our approach on a large cohort of glioblastoma patients stratified by age groups. Results demonstrate significant differences in radiomics features across age groups, with distinct patterns emerging for young, middle-aged, and elderly patients. These differences correlate with variations in survival outcomes and treatment responses.\nThe findings from this study provide valuable insights into age-related glioblastoma characteristics and could inform the development of age-specific treatment strategies and prognostic models."
  },
  {
    "objectID": "publication/2017-09-01-age-groups-glioblastoma/index.html#abstract",
    "href": "publication/2017-09-01-age-groups-glioblastoma/index.html#abstract",
    "title": "Age Groups Related Glioblastoma Study Based on Radiomics Approach",
    "section": "",
    "text": "Glioblastoma exhibits different characteristics and outcomes across different age groups, making age-specific analysis crucial for personalized treatment strategies. Traditional approaches often treat glioblastoma as a homogeneous disease, failing to capture the age-related variations in tumor biology and patient outcomes. In this work, we present a comprehensive radiomics-based study investigating age-related differences in glioblastoma characteristics.\nOur approach leverages quantitative imaging features to characterize glioblastoma across different age groups, from young adults to elderly patients. The study analyzes multiple radiomics features including texture, shape, and intensity-based descriptors extracted from brain MRI scans. We investigate how these features vary across age groups and their correlation with clinical outcomes.\nWe evaluate our approach on a large cohort of glioblastoma patients stratified by age groups. Results demonstrate significant differences in radiomics features across age groups, with distinct patterns emerging for young, middle-aged, and elderly patients. These differences correlate with variations in survival outcomes and treatment responses.\nThe findings from this study provide valuable insights into age-related glioblastoma characteristics and could inform the development of age-specific treatment strategies and prognostic models."
  },
  {
    "objectID": "publication/2020-06-01-high-resolution-chest-xray-bone-suppression/index.html",
    "href": "publication/2020-06-01-high-resolution-chest-xray-bone-suppression/index.html",
    "title": "High-Resolution Chest X-ray Bone Suppression Using Unpaired CT Structural Priors",
    "section": "",
    "text": "Chest X-ray bone suppression is crucial for improving the visibility of soft tissue structures and enhancing diagnostic accuracy. However, traditional approaches often struggle with maintaining high resolution and anatomical accuracy. In this work, we propose a novel approach for high-resolution chest X-ray bone suppression using unpaired CT structural priors.\nOur method leverages the rich structural information available in CT scans to guide bone suppression in X-ray images, even without paired training data. The framework consists of three main components: (1) a structural prior extraction module that learns anatomical knowledge from CT scans, (2) an unpaired learning framework that transfers structural knowledge to X-ray domain, and (3) a high-resolution generation module that produces detailed bone-suppressed images.\nWe evaluate our approach on a large dataset of chest X-ray images with expert annotations. Experimental results demonstrate that our method significantly improves bone suppression quality compared to existing approaches, particularly in high-resolution scenarios. The method maintains anatomical accuracy while providing enhanced soft tissue visibility.\nThe proposed framework represents a significant advancement in chest X-ray processing, providing more effective bone suppression that could improve diagnostic accuracy in clinical practice."
  },
  {
    "objectID": "publication/2020-06-01-high-resolution-chest-xray-bone-suppression/index.html#abstract",
    "href": "publication/2020-06-01-high-resolution-chest-xray-bone-suppression/index.html#abstract",
    "title": "High-Resolution Chest X-ray Bone Suppression Using Unpaired CT Structural Priors",
    "section": "",
    "text": "Chest X-ray bone suppression is crucial for improving the visibility of soft tissue structures and enhancing diagnostic accuracy. However, traditional approaches often struggle with maintaining high resolution and anatomical accuracy. In this work, we propose a novel approach for high-resolution chest X-ray bone suppression using unpaired CT structural priors.\nOur method leverages the rich structural information available in CT scans to guide bone suppression in X-ray images, even without paired training data. The framework consists of three main components: (1) a structural prior extraction module that learns anatomical knowledge from CT scans, (2) an unpaired learning framework that transfers structural knowledge to X-ray domain, and (3) a high-resolution generation module that produces detailed bone-suppressed images.\nWe evaluate our approach on a large dataset of chest X-ray images with expert annotations. Experimental results demonstrate that our method significantly improves bone suppression quality compared to existing approaches, particularly in high-resolution scenarios. The method maintains anatomical accuracy while providing enhanced soft tissue visibility.\nThe proposed framework represents a significant advancement in chest X-ray processing, providing more effective bone suppression that could improve diagnostic accuracy in clinical practice."
  },
  {
    "objectID": "publication/2022-09-01-improved-post-hoc-probability-calibration/index.html",
    "href": "publication/2022-09-01-improved-post-hoc-probability-calibration/index.html",
    "title": "Improved Post-Hoc Probability Calibration for Artifact-Corrupted MRI Segmentation",
    "section": "",
    "text": "Probability calibration is crucial for reliable medical image segmentation, particularly when dealing with artifact-corrupted images that can significantly impact model confidence estimates. Traditional calibration methods often fail to account for the specific challenges posed by imaging artifacts. In this work, we propose an improved post-hoc probability calibration approach specifically designed for artifact-corrupted MRI segmentation.\nOur method introduces a calibration framework that accounts for the presence of imaging artifacts and their impact on model predictions. The framework consists of three key components: (1) an artifact detection module that identifies corrupted regions, (2) a calibration module that adjusts probability estimates based on artifact presence, and (3) a confidence refinement mechanism that provides more reliable uncertainty estimates.\nWe evaluate our approach on MRI datasets with various types of artifacts, including motion artifacts and noise. Experimental results demonstrate that our artifact-aware calibration approach significantly improves probability calibration compared to traditional methods. The method shows excellent performance in providing reliable confidence estimates even in the presence of artifacts.\nThe proposed framework represents a significant advancement in probability calibration for medical imaging, providing more reliable uncertainty estimates that could improve clinical decision-making and model interpretability."
  },
  {
    "objectID": "publication/2022-09-01-improved-post-hoc-probability-calibration/index.html#abstract",
    "href": "publication/2022-09-01-improved-post-hoc-probability-calibration/index.html#abstract",
    "title": "Improved Post-Hoc Probability Calibration for Artifact-Corrupted MRI Segmentation",
    "section": "",
    "text": "Probability calibration is crucial for reliable medical image segmentation, particularly when dealing with artifact-corrupted images that can significantly impact model confidence estimates. Traditional calibration methods often fail to account for the specific challenges posed by imaging artifacts. In this work, we propose an improved post-hoc probability calibration approach specifically designed for artifact-corrupted MRI segmentation.\nOur method introduces a calibration framework that accounts for the presence of imaging artifacts and their impact on model predictions. The framework consists of three key components: (1) an artifact detection module that identifies corrupted regions, (2) a calibration module that adjusts probability estimates based on artifact presence, and (3) a confidence refinement mechanism that provides more reliable uncertainty estimates.\nWe evaluate our approach on MRI datasets with various types of artifacts, including motion artifacts and noise. Experimental results demonstrate that our artifact-aware calibration approach significantly improves probability calibration compared to traditional methods. The method shows excellent performance in providing reliable confidence estimates even in the presence of artifacts.\nThe proposed framework represents a significant advancement in probability calibration for medical imaging, providing more reliable uncertainty estimates that could improve clinical decision-making and model interpretability."
  },
  {
    "objectID": "publication/2017-12-01-deep-learning-radiomics-idh1/index.html",
    "href": "publication/2017-12-01-deep-learning-radiomics-idh1/index.html",
    "title": "Deep Learning based Radiomics (DLR) and its Usage in Noninvasive IDH1 Prediction for Low Grade Glioma",
    "section": "",
    "text": "Isocitrate dehydrogenase 1 (IDH1) mutation status is a critical prognostic factor in low-grade gliomas, traditionally requiring invasive biopsy for determination. Noninvasive prediction of IDH1 status could significantly improve patient management and treatment planning. In this work, we propose Deep Learning based Radiomics (DLR), a novel approach that combines deep learning with traditional radiomics for noninvasive IDH1 prediction.\nOur DLR framework integrates convolutional neural networks with radiomics features to capture both high-level image patterns and quantitative imaging biomarkers. The approach consists of three main components: (1) a deep learning module that extracts hierarchical features from brain MRI scans, (2) a radiomics feature extraction module that computes quantitative imaging descriptors, and (3) a fusion module that combines both feature types for final prediction.\nWe evaluate our method on a comprehensive dataset of low-grade glioma patients with known IDH1 mutation status. Experimental results demonstrate that DLR significantly outperforms traditional radiomics approaches and deep learning methods alone, achieving high accuracy in IDH1 prediction. The method shows robust performance across different MRI sequences and patient populations.\nDLR represents a significant advancement in noninvasive glioma characterization, providing a more accurate and clinically practical approach to IDH1 prediction that could reduce the need for invasive procedures and improve patient outcomes."
  },
  {
    "objectID": "publication/2017-12-01-deep-learning-radiomics-idh1/index.html#abstract",
    "href": "publication/2017-12-01-deep-learning-radiomics-idh1/index.html#abstract",
    "title": "Deep Learning based Radiomics (DLR) and its Usage in Noninvasive IDH1 Prediction for Low Grade Glioma",
    "section": "",
    "text": "Isocitrate dehydrogenase 1 (IDH1) mutation status is a critical prognostic factor in low-grade gliomas, traditionally requiring invasive biopsy for determination. Noninvasive prediction of IDH1 status could significantly improve patient management and treatment planning. In this work, we propose Deep Learning based Radiomics (DLR), a novel approach that combines deep learning with traditional radiomics for noninvasive IDH1 prediction.\nOur DLR framework integrates convolutional neural networks with radiomics features to capture both high-level image patterns and quantitative imaging biomarkers. The approach consists of three main components: (1) a deep learning module that extracts hierarchical features from brain MRI scans, (2) a radiomics feature extraction module that computes quantitative imaging descriptors, and (3) a fusion module that combines both feature types for final prediction.\nWe evaluate our method on a comprehensive dataset of low-grade glioma patients with known IDH1 mutation status. Experimental results demonstrate that DLR significantly outperforms traditional radiomics approaches and deep learning methods alone, achieving high accuracy in IDH1 prediction. The method shows robust performance across different MRI sequences and patient populations.\nDLR represents a significant advancement in noninvasive glioma characterization, providing a more accurate and clinically practical approach to IDH1 prediction that could reduce the need for invasive procedures and improve patient outcomes."
  },
  {
    "objectID": "publication/2022-11-01-causality-inspired-domain-generalization/index.html",
    "href": "publication/2022-11-01-causality-inspired-domain-generalization/index.html",
    "title": "Causality-inspired Single-source Domain Generalization for Medical Image Segmentation",
    "section": "",
    "text": "Domain generalization is a critical challenge in medical image segmentation, where models trained on one domain often fail to generalize to unseen domains. Traditional approaches typically require multiple source domains, which may not always be available in clinical settings. In this work, we propose a causality-inspired approach for single-source domain generalization in medical image segmentation.\nOur method leverages causal inference principles to identify and preserve domain-invariant features while learning domain-specific representations. The framework consists of three main components: (1) a causal feature extraction module that identifies stable causal relationships, (2) a domain-invariant learning module that preserves causal features across domains, and (3) a domain-specific adaptation module that handles domain-specific variations.\nWe evaluate our approach on multiple medical image segmentation datasets with domain shifts, including cross-scanner and cross-institution scenarios. Experimental results demonstrate that our causality-inspired approach significantly outperforms existing domain generalization methods, particularly in single-source scenarios. The method shows robust performance across different types of domain shifts and maintains segmentation accuracy in unseen domains.\nThe proposed framework represents a significant advancement in domain generalization for medical imaging, providing a more principled approach that could improve the deployment of segmentation models across different clinical settings."
  },
  {
    "objectID": "publication/2022-11-01-causality-inspired-domain-generalization/index.html#abstract",
    "href": "publication/2022-11-01-causality-inspired-domain-generalization/index.html#abstract",
    "title": "Causality-inspired Single-source Domain Generalization for Medical Image Segmentation",
    "section": "",
    "text": "Domain generalization is a critical challenge in medical image segmentation, where models trained on one domain often fail to generalize to unseen domains. Traditional approaches typically require multiple source domains, which may not always be available in clinical settings. In this work, we propose a causality-inspired approach for single-source domain generalization in medical image segmentation.\nOur method leverages causal inference principles to identify and preserve domain-invariant features while learning domain-specific representations. The framework consists of three main components: (1) a causal feature extraction module that identifies stable causal relationships, (2) a domain-invariant learning module that preserves causal features across domains, and (3) a domain-specific adaptation module that handles domain-specific variations.\nWe evaluate our approach on multiple medical image segmentation datasets with domain shifts, including cross-scanner and cross-institution scenarios. Experimental results demonstrate that our causality-inspired approach significantly outperforms existing domain generalization methods, particularly in single-source scenarios. The method shows robust performance across different types of domain shifts and maintains segmentation accuracy in unseen domains.\nThe proposed framework represents a significant advancement in domain generalization for medical imaging, providing a more principled approach that could improve the deployment of segmentation models across different clinical settings."
  },
  {
    "objectID": "publication/2025-09-01-vap-diffusion-enriching-descriptions-mllms/index.html",
    "href": "publication/2025-09-01-vap-diffusion-enriching-descriptions-mllms/index.html",
    "title": "VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation",
    "section": "",
    "text": "Medical image generation has significant potential for data augmentation, training, and educational purposes. However, generating realistic and anatomically accurate medical images remains challenging, particularly when using text descriptions that may lack detailed anatomical information. In this work, we propose VAP-Diffusion, a novel approach that leverages multimodal large language models (MLLMs) to enrich descriptions for enhanced medical image generation.\nOur method introduces a description enrichment framework that uses MLLMs to enhance text prompts with detailed anatomical and pathological information. The framework consists of three key components: (1) an MLLM-based description enrichment module that adds anatomical details to input prompts, (2) a diffusion-based image generation module that produces high-quality medical images, and (3) an anatomical consistency module that ensures generated images maintain clinical relevance.\nWe evaluate our approach on multiple medical imaging datasets, including brain MRI and chest X-ray images. Experimental results demonstrate that our MLLM-enriched approach significantly improves image quality and anatomical accuracy compared to traditional text-to-image generation methods. The method shows excellent performance in generating diverse and clinically relevant medical images.\nThe proposed framework represents a significant advancement in medical image generation, providing more accurate and useful synthetic data that could improve AI model training and clinical education."
  },
  {
    "objectID": "publication/2025-09-01-vap-diffusion-enriching-descriptions-mllms/index.html#abstract",
    "href": "publication/2025-09-01-vap-diffusion-enriching-descriptions-mllms/index.html#abstract",
    "title": "VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation",
    "section": "",
    "text": "Medical image generation has significant potential for data augmentation, training, and educational purposes. However, generating realistic and anatomically accurate medical images remains challenging, particularly when using text descriptions that may lack detailed anatomical information. In this work, we propose VAP-Diffusion, a novel approach that leverages multimodal large language models (MLLMs) to enrich descriptions for enhanced medical image generation.\nOur method introduces a description enrichment framework that uses MLLMs to enhance text prompts with detailed anatomical and pathological information. The framework consists of three key components: (1) an MLLM-based description enrichment module that adds anatomical details to input prompts, (2) a diffusion-based image generation module that produces high-quality medical images, and (3) an anatomical consistency module that ensures generated images maintain clinical relevance.\nWe evaluate our approach on multiple medical imaging datasets, including brain MRI and chest X-ray images. Experimental results demonstrate that our MLLM-enriched approach significantly improves image quality and anatomical accuracy compared to traditional text-to-image generation methods. The method shows excellent performance in generating diverse and clinically relevant medical images.\nThe proposed framework represents a significant advancement in medical image generation, providing more accurate and useful synthetic data that could improve AI model training and clinical education."
  },
  {
    "objectID": "publication/2017-09-01-reconstruction-thin-slice-medical-images-gan/index.html",
    "href": "publication/2017-09-01-reconstruction-thin-slice-medical-images-gan/index.html",
    "title": "Reconstruction of Thin-Slice Medical Images Using Generative Adversarial Network",
    "section": "",
    "text": "Thin-slice medical image reconstruction is crucial for improving diagnostic accuracy, particularly when high-resolution scans are unavailable or costly to acquire. Traditional reconstruction methods often struggle with maintaining image quality and anatomical accuracy. In this work, we propose a generative adversarial network approach for reconstructing high-quality thin-slice medical images.\nOur method leverages the power of generative adversarial networks to produce high-resolution images from limited or noisy data. The framework consists of three key components: (1) a generator network that learns to reconstruct high-quality images, (2) a discriminator network that ensures realistic output, and (3) a specialized loss function that preserves anatomical details and structural integrity.\nWe evaluate our approach on multiple medical imaging datasets, including brain MRI and chest CT scans. Experimental results demonstrate that our GAN-based method significantly improves reconstruction quality compared to traditional approaches. The method shows excellent performance in preserving fine anatomical details and reducing artifacts commonly found in thin-slice imaging.\nThe proposed framework represents a significant advancement in medical image reconstruction, providing higher quality images that could improve diagnostic accuracy and reduce the need for additional imaging procedures."
  },
  {
    "objectID": "publication/2017-09-01-reconstruction-thin-slice-medical-images-gan/index.html#abstract",
    "href": "publication/2017-09-01-reconstruction-thin-slice-medical-images-gan/index.html#abstract",
    "title": "Reconstruction of Thin-Slice Medical Images Using Generative Adversarial Network",
    "section": "",
    "text": "Thin-slice medical image reconstruction is crucial for improving diagnostic accuracy, particularly when high-resolution scans are unavailable or costly to acquire. Traditional reconstruction methods often struggle with maintaining image quality and anatomical accuracy. In this work, we propose a generative adversarial network approach for reconstructing high-quality thin-slice medical images.\nOur method leverages the power of generative adversarial networks to produce high-resolution images from limited or noisy data. The framework consists of three key components: (1) a generator network that learns to reconstruct high-quality images, (2) a discriminator network that ensures realistic output, and (3) a specialized loss function that preserves anatomical details and structural integrity.\nWe evaluate our approach on multiple medical imaging datasets, including brain MRI and chest CT scans. Experimental results demonstrate that our GAN-based method significantly improves reconstruction quality compared to traditional approaches. The method shows excellent performance in preserving fine anatomical details and reducing artifacts commonly found in thin-slice imaging.\nThe proposed framework represents a significant advancement in medical image reconstruction, providing higher quality images that could improve diagnostic accuracy and reduce the need for additional imaging procedures."
  },
  {
    "objectID": "publication/2019-01-01-deep-generative-adversarial-networks-infant-mr/index.html",
    "href": "publication/2019-01-01-deep-generative-adversarial-networks-infant-mr/index.html",
    "title": "Deep Generative Adversarial Networks for Thinsection Infant MR Image Reconstruction",
    "section": "",
    "text": "Infant brain MRI reconstruction presents unique challenges due to the rapid development and small size of infant brains, often requiring thin-section imaging that can result in noisy or incomplete data. Traditional reconstruction methods often struggle with the specific characteristics of infant brain imaging. In this work, we propose a novel deep generative adversarial network approach for thinsection infant MR image reconstruction.\nOur method leverages the power of generative adversarial networks to produce high-quality reconstructed images from limited or noisy infant brain MRI data. The framework consists of three main components: (1) a generator network that learns to reconstruct high-quality images, (2) a discriminator network that ensures realistic output, and (3) a specialized loss function that preserves infant brain characteristics.\nWe evaluate our approach on a comprehensive dataset of infant brain MRI scans with various imaging protocols. Experimental results demonstrate that our GAN-based method significantly improves image quality compared to traditional reconstruction approaches. The method shows particular effectiveness in handling the unique challenges of infant brain imaging, including rapid developmental changes and small anatomical structures.\nThe proposed framework represents a significant advancement in infant brain MRI processing, providing higher quality images that could improve diagnostic accuracy and research applications in pediatric neuroimaging."
  },
  {
    "objectID": "publication/2019-01-01-deep-generative-adversarial-networks-infant-mr/index.html#abstract",
    "href": "publication/2019-01-01-deep-generative-adversarial-networks-infant-mr/index.html#abstract",
    "title": "Deep Generative Adversarial Networks for Thinsection Infant MR Image Reconstruction",
    "section": "",
    "text": "Infant brain MRI reconstruction presents unique challenges due to the rapid development and small size of infant brains, often requiring thin-section imaging that can result in noisy or incomplete data. Traditional reconstruction methods often struggle with the specific characteristics of infant brain imaging. In this work, we propose a novel deep generative adversarial network approach for thinsection infant MR image reconstruction.\nOur method leverages the power of generative adversarial networks to produce high-quality reconstructed images from limited or noisy infant brain MRI data. The framework consists of three main components: (1) a generator network that learns to reconstruct high-quality images, (2) a discriminator network that ensures realistic output, and (3) a specialized loss function that preserves infant brain characteristics.\nWe evaluate our approach on a comprehensive dataset of infant brain MRI scans with various imaging protocols. Experimental results demonstrate that our GAN-based method significantly improves image quality compared to traditional reconstruction approaches. The method shows particular effectiveness in handling the unique challenges of infant brain imaging, including rapid developmental changes and small anatomical structures.\nThe proposed framework represents a significant advancement in infant brain MRI processing, providing higher quality images that could improve diagnostic accuracy and research applications in pediatric neuroimaging."
  },
  {
    "objectID": "publication/2019-03-01-early-identification-ischemic-stroke/index.html",
    "href": "publication/2019-03-01-early-identification-ischemic-stroke/index.html",
    "title": "Early Identification of Ischemic Stroke in Noncontrast Computed Tomography",
    "section": "",
    "text": "Early identification of ischemic stroke is critical for timely intervention and improved patient outcomes. However, detecting early signs of stroke in noncontrast computed tomography (CT) remains challenging due to subtle appearance changes and limited contrast. In this work, we propose a novel approach for early identification of ischemic stroke in noncontrast CT images.\nOur method leverages advanced image processing and machine learning techniques to detect subtle signs of ischemia that may be missed by human observers. The framework consists of three key components: (1) a feature extraction module that identifies early ischemic changes, (2) a temporal analysis module that tracks progression over time, and (3) a classification module that provides early stroke detection.\nWe evaluate our approach on a comprehensive dataset of stroke patients with confirmed diagnoses and follow-up imaging. Experimental results demonstrate that our method significantly improves early stroke detection compared to traditional approaches. The method shows particular effectiveness in detecting subtle signs of ischemia that may precede obvious clinical manifestations.\nThe proposed framework represents a significant advancement in stroke detection, providing earlier identification that could improve treatment timing and patient outcomes."
  },
  {
    "objectID": "publication/2019-03-01-early-identification-ischemic-stroke/index.html#abstract",
    "href": "publication/2019-03-01-early-identification-ischemic-stroke/index.html#abstract",
    "title": "Early Identification of Ischemic Stroke in Noncontrast Computed Tomography",
    "section": "",
    "text": "Early identification of ischemic stroke is critical for timely intervention and improved patient outcomes. However, detecting early signs of stroke in noncontrast computed tomography (CT) remains challenging due to subtle appearance changes and limited contrast. In this work, we propose a novel approach for early identification of ischemic stroke in noncontrast CT images.\nOur method leverages advanced image processing and machine learning techniques to detect subtle signs of ischemia that may be missed by human observers. The framework consists of three key components: (1) a feature extraction module that identifies early ischemic changes, (2) a temporal analysis module that tracks progression over time, and (3) a classification module that provides early stroke detection.\nWe evaluate our approach on a comprehensive dataset of stroke patients with confirmed diagnoses and follow-up imaging. Experimental results demonstrate that our method significantly improves early stroke detection compared to traditional approaches. The method shows particular effectiveness in detecting subtle signs of ischemia that may precede obvious clinical manifestations.\nThe proposed framework represents a significant advancement in stroke detection, providing earlier identification that could improve treatment timing and patient outcomes."
  },
  {
    "objectID": "publication/index.html",
    "href": "publication/index.html",
    "title": "Publications",
    "section": "",
    "text": "VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation\n\n\n\n2025\n\n\n\n\n\n\nSep 1, 2025\n\n\nPeng Huang, Junhu Fu, Bowen Guo, Zeju Li, Yuanyuan Wang, Yi Guo\n\n\n\n\n\n\n\nFedFDD: Federated Learning with Frequency Domain Decomposition for Low-Dose CT Denoising\n\n\n\n2024\n\n\n\n\n\n\nJul 1, 2024\n\n\nXuhang Chen, Zeju Li, Zikun Xu, Kaijie Xu, Cheng Ouyang, Chen Qin\n\n\n\n\n\n\n\nPost-Deployment Adaptation with Access to Source Data via Federated Learning and Source-Target Remote Gradient Alignment\n\n\n\n2023\n\n\n\n\n\n\nSep 1, 2023\n\n\nFelix Wagner, Zeju Li, Pramit Saha, Konstantinos Kamnitsas\n\n\n\n\n\n\n\nRobust Segmentation via Topology Violation Detection and Feature Synthesis\n\n\n\n2023\n\n\n\n\n\n\nSep 1, 2023\n\n\nLiu Li, Qiang Ma, Cheng Ouyang, Zeju Li, Qingjie Meng, Weitong Zhang, Mengyun Qiao, Vanessa Kyriakopoulou, Joseph Hajnal, Daniel Rueckert, Bernhard Kainz\n\n\n\n\n\n\n\nRobustness Stress Testing in Medical Image Classification\n\n\n\n2023\n\n\n\n\n\n\nSep 1, 2023\n\n\nMobarakol Islam, Zeju Li, Ben Glocker\n\n\n\n\n\n\n\nContext Label Learning: Improving Background Class Representations in Semantic Segmentation\n\n\n\nhighlights\n\n2023\n\n\n\n\n\n\nJun 27, 2023\n\n\nZeju Li, Konstantinos Kamnitsas, Cheng Ouyang, Chen Chen, Ben Glocker\n\n\n\n\n\n\n\nJoint Optimization of Class-Specific Training- and Test-Time Data Augmentation in Segmentation\n\n\n\nhighlights\n\n2023\n\n\n\n\n\n\nJun 27, 2023\n\n\nZeju Li, Konstantinos Kamnitsas, Qi Dou, Chen Qin, Ben Glocker\n\n\n\n\n\n\n\nCausality-inspired Single-source Domain Generalization for Medical Image Segmentation\n\n\n\n2022\n\n\n\n\n\n\nNov 1, 2022\n\n\nCheng Ouyang, Chen Chen, Surui Li, Zeju Li, Chen Qin, Wenjia Bai, Daniel Rueckert\n\n\n\n\n\n\n\nTackling Long-Tailed Category Distribution Under Domain Shifts\n\n\n\n2022\n\n\n\n\n\n\nOct 1, 2022\n\n\nXiao Gu, Yao Guo, Zeju Li, Jianning Qiu, Qi Dou, Yuxuan Liu, Benny Lo, Guang-Zhong Yang\n\n\n\n\n\n\n\nEstimating Model Performance under Domain Shifts with Class-Specific Confidence Scores\n\n\n\nhighlights\n\n2022\n\n\n\n\n\n\nSep 1, 2022\n\n\nZeju Li, Konstantinos Kamnitsas, Mobarakol Islam, Chen Chen, Ben Glocker\n\n\n\n\n\n\n\nFetal Cortex Segmentation with Topology and Thickness Loss Constraints\n\n\n\n2022\n\n\n\n\n\n\nSep 1, 2022\n\n\nLiu Li, Qiang Ma, Zeju Li, Cheng Ouyang, Weitong Zhang, Anthony Price, Vanessa Kyriakopoulou, Lucilio Cordero-Grande, Antonis Makropoulos, Joseph Hajnal, Daniel Rueckert, Bernhard Kainz, Amir Alansary\n\n\n\n\n\n\n\nImproved Post-Hoc Probability Calibration for Artifact-Corrupted MRI Segmentation\n\n\n\n2022\n\n\n\n\n\n\nSep 1, 2022\n\n\nCheng Ouyang, Shuo Wang, Chen Chen, Zeju Li, Wenjia Bai, Bernhard Kainz, Daniel Rueckert\n\n\n\n\n\n\n\nMaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation\n\n\n\n2022\n\n\n\n\n\n\nSep 1, 2022\n\n\nChen Chen, Zeju Li, Cheng Ouyang, Matt Sinclair, Wenjia Bai, Daniel Rueckert\n\n\n\n\n\n\n\nEnhancing MR Image Segmentation with Realistic Adversarial Data Augmentation\n\n\n\n2022\n\n\n\n\n\n\nJun 1, 2022\n\n\nChen Chen, Cheng Ouyang, Zeju Li, Shuo Wang, Huaqi Qiu, Liang Chen, Giacomo Tarroni, Wenjia Bai, Daniel Rueckert\n\n\n\n\n\n\n\nLearn2Reg: Comprehensive Multi-Task Medical Image Registration Challenge, Dataset and Evaluation in the Era of Deep Learning\n\n\n\n2022\n\n\n\n\n\n\nMar 1, 2022\n\n\nAlessa Hering, Lasse Hansen, Tony CW Mok, Albert CS Chung, Hanna Siebert, Stephanie H√§ger, Annkristin Lange, Sven Kuckertz, Stefan Heldmann, Wei Shao, others\n\n\n\n\n\n\n\nBreast Tumor Classification based on MRI-US Images by Disentangling Modality Features\n\n\n\n2022\n\n\n\n\n\n\nJan 15, 2022\n\n\nMengyun Qiao, Chengcheng Liu, Zeju Li, Jin Zhou, Qin Xiao, Shichong Zhou, Cai Chang, Gu Yajia, Yi Guo, Yuanyuan Wang\n\n\n\n\n\n\n\nFederated Deep Learning for Detecting COVID-19 Lung Abnormalities in CT: A Privacy-Preserving Multinational Validation Study\n\n\n\n2021\n\n\n\n\n\n\nDec 1, 2021\n\n\nQi Dou, Tiffany Y So, Meirui Jiang, Quande Liu, Varut Vardhanabhuti, Georgios Kaissis, Zeju Li, Weixin Si, Heather HC Lee, Kevin Yu, others\n\n\n\n\n\n\n\nA Novel Image Signature-Based Radiomics Method to Achieve Precise Diagnosis and Prognostic Stratification of Gliomas\n\n\n\n2021\n\n\n\n\n\n\nApr 1, 2021\n\n\nHuigao Luo, Qiyuan Zhuang, Yuanyuan Wang, Aibaidula Abudumijiti, Kuangyu Shi, Axel Rominger, Hong Chen, Zhong Yang, Vanessa Tran, Guoqing Wu, others\n\n\n\n\n\n\n\nAnalyzing Overfitting under Class Imbalance in Neural Networks for Image Segmentation\n\n\n\nhighlights\n\n2020\n\n\n\n\n\n\nDec 17, 2020\n\n\nZeju Li, Konstantinos Kamnitsas, Ben Glocker\n\n\n\n\n\n\n\nHigh-Resolution Chest X-ray Bone Suppression Using Unpaired CT Structural Priors\n\n\n\n2020\n\n\n\n\n\n\nJun 1, 2020\n\n\nHan Li, Hu Han, Zeju Li, Lei Wang, Zhe Wu, Jingjing Lu, S Kevin Zhou\n\n\n\n\n\n\n\nDeepVolume: Brain Structure and Spatial Connection-Aware Network for Brain MRI Super-Resolution\n\n\n\nhighlights\n\n2019\n\n\n\n\n\n\nDec 1, 2019\n\n\nZeju Li, Jinhua Yu, Yuanyuan Wang, Hanzhang Zhou, Haowei Yang, Zhuowei Qiao\n\n\n\n\n\n\n\nEncoding CT Anatomy Knowledge for Unpaired Chest X-ray Image Decomposition\n\n\n\nhighlights\n\n2019\n\n\n\n\n\n\nOct 1, 2019\n\n\nZeju Li, Han Li, Hu Han, Gonglei Shi, Jiannan Wang, Shaohua Kervin Zhou\n\n\n\n\n\n\n\nOverfitting of Neural Nets Under Class Imbalance: Analysis and Improvements for Segmentation\n\n\n\n2019\n\n\n\n\n\n\nOct 1, 2019\n\n\nZeju Li, Konstantinos Kamnitsas, Ben Glocker\n\n\n\n\n\n\n\nEarly Identification of Ischemic Stroke in Noncontrast Computed Tomography\n\n\n\n2019\n\n\n\n\n\n\nMar 1, 2019\n\n\nGuoqing Wu, Jixian Lin, Xi Chen, Zeju Li, Yuanyuan Wang, Jing Zhao, Jinhua Yu\n\n\n\n\n\n\n\nDeep Generative Adversarial Networks for Thinsection Infant MR Image Reconstruction\n\n\n\n2019\n\n\n\n\n\n\nJan 1, 2019\n\n\nJiaqi Gu, Zeju Li, Yuanyuan Wang, Haowei Yang, Zhongwei Qiao, Jinhua Yu\n\n\n\n\n\n\n\nSuper-Resolution Reconstruction of Plane-Wave Ultrasound Image Based on a Multi-Angle Parallel U-Net with Maxout Unit and Novel Loss Function\n\n\n\n2019\n\n\n\n\n\n\nJan 1, 2019\n\n\nZixia Zhou, Yuanyuan Wang, Jinhua Yu, Wei Guo, Zeju Li\n\n\n\n\n\n\n\nPrimary Central Nervous System Lymphoma and Glioblastoma Differentiation Based on Conventional Magnetic Resonance Imaging by High-Throughput SIFT Features\n\n\n\n2018\n\n\n\n\n\n\nDec 1, 2018\n\n\nYinsheng Chen, Zeju Li, Guoqing Wu, Jinhua Yu, Yuanyuan Wang, Xiaofei Lv, Xue Ju, Zhongping Chen\n\n\n\n\n\n\n\nLeft Ventricle Segmentation via Optical-Flow-Net from Short-Axis Cine MRI: Preserving the Temporal Coherence of Cardiac Motion\n\n\n\n2018\n\n\n\n\n\n\nSep 1, 2018\n\n\nWenjun Yan, Yuanyuan Wang, Zeju Li, Rob J van der Geest, Qian Tao\n\n\n\n\n\n\n\nA Multi-Scope Convolutional Neural Network for Automatic Left Ventricle Segmentation from Magnetic Resonance Images: Deep-Learning at Multiple Scopes\n\n\n\n2018\n\n\n\n\n\n\nJul 1, 2018\n\n\nXinyi Li, Yuanyuan Wang, Wenjun Yan, Rob J Van der Geest, Zeju Li, Qian Tao\n\n\n\n\n\n\n\nPrimary Central Nervous System Lymphoma and Glioblastoma Image Differentiation Based on Sparse Representation System\n\n\n\n2018\n\n\n\n\n\n\nJan 1, 2018\n\n\nGuoqing Wu, Zeju Li, Yuanyuan Wang, Jinhua Yu, Yinsheng Chen, Zhongping Chen\n\n\n\n\n\n\n\nDeep Learning based Radiomics (DLR) and its Usage in Noninvasive IDH1 Prediction for Low Grade Glioma\n\n\n\n2017\n\n\n\n\n\n\nDec 1, 2017\n\n\nZeju Li, Yuanyuan Wang, Jinhua Yu, Yi Guo, Wei Cao\n\n\n\n\n\n\n\nNoninvasive IDH1 Mutation Estimation Based on a Quantitative Radiomics Approach for Grade II Glioma\n\n\n\n2017\n\n\n\n\n\n\nDec 1, 2017\n\n\nJinhua Yu, Zhifeng Shi, Yuxi Lian, Zeju Li, Tongtong Liu, Yuan Gao, Yuanyuan Wang, Liang Chen, Ying Mao\n\n\n\n\n\n\n\nLow-Grade Glioma Segmentation Based on CNN with Fully Connected CRF\n\n\n\n2017\n\n\n\n\n\n\nOct 1, 2017\n\n\nZeju Li, Yuanyuan Wang, Jinhua Yu, Zhifeng Shi, Yi Guo, Liang Chen, Ying Mao\n\n\n\n\n\n\n\nAge Groups Related Glioblastoma Study Based on Radiomics Approach\n\n\n\n2017\n\n\n\n\n\n\nSep 1, 2017\n\n\nZeju Li, Yuanyuan Wang, Jinhua Yu, Yi Guo, Qi Zhang\n\n\n\n\n\n\n\nBrain Tumor Segmentation Using an Adversarial Network\n\n\n\n2017\n\n\n\n\n\n\nSep 1, 2017\n\n\nZeju Li, Yuanyuan Wang, Jinhua Yu\n\n\n\n\n\n\n\nReconstruction of Thin-Slice Medical Images Using Generative Adversarial Network\n\n\n\n2017\n\n\n\n\n\n\nSep 1, 2017\n\n\nZeju Li, Yuanyuan Wang, Jinhua Yu\n\n\n\n\n\nNo matching items\n\n  \n\n Back to top"
  },
  {
    "objectID": "publication/2018-12-01-primary-cns-lymphoma-glioblastoma-differentiation/index.html",
    "href": "publication/2018-12-01-primary-cns-lymphoma-glioblastoma-differentiation/index.html",
    "title": "Primary Central Nervous System Lymphoma and Glioblastoma Differentiation Based on Conventional Magnetic Resonance Imaging by High-Throughput SIFT Features",
    "section": "",
    "text": "Differentiating primary central nervous system lymphoma (PCNSL) from glioblastoma is crucial for treatment planning, as these conditions require different therapeutic approaches. However, distinguishing between them using conventional MRI remains challenging due to overlapping imaging characteristics. In this work, we propose a high-throughput SIFT feature-based approach for automated differentiation of PCNSL from glioblastoma.\nOur method leverages Scale-Invariant Feature Transform (SIFT) features to capture distinctive imaging patterns that may be subtle to human observers. The framework consists of three key components: (1) a high-throughput SIFT feature extraction module that identifies distinctive patterns, (2) a feature selection mechanism that identifies the most discriminative features, and (3) a classification module that provides automated differentiation.\nWe evaluate our approach on a comprehensive dataset of PCNSL and glioblastoma cases with confirmed diagnoses. Experimental results demonstrate that our SIFT-based method significantly improves differentiation accuracy compared to traditional approaches. The method shows robust performance across different imaging protocols and patient populations.\nThe proposed framework represents a significant advancement in brain tumor differentiation, providing more accurate classification that could improve treatment planning and patient outcomes."
  },
  {
    "objectID": "publication/2018-12-01-primary-cns-lymphoma-glioblastoma-differentiation/index.html#abstract",
    "href": "publication/2018-12-01-primary-cns-lymphoma-glioblastoma-differentiation/index.html#abstract",
    "title": "Primary Central Nervous System Lymphoma and Glioblastoma Differentiation Based on Conventional Magnetic Resonance Imaging by High-Throughput SIFT Features",
    "section": "",
    "text": "Differentiating primary central nervous system lymphoma (PCNSL) from glioblastoma is crucial for treatment planning, as these conditions require different therapeutic approaches. However, distinguishing between them using conventional MRI remains challenging due to overlapping imaging characteristics. In this work, we propose a high-throughput SIFT feature-based approach for automated differentiation of PCNSL from glioblastoma.\nOur method leverages Scale-Invariant Feature Transform (SIFT) features to capture distinctive imaging patterns that may be subtle to human observers. The framework consists of three key components: (1) a high-throughput SIFT feature extraction module that identifies distinctive patterns, (2) a feature selection mechanism that identifies the most discriminative features, and (3) a classification module that provides automated differentiation.\nWe evaluate our approach on a comprehensive dataset of PCNSL and glioblastoma cases with confirmed diagnoses. Experimental results demonstrate that our SIFT-based method significantly improves differentiation accuracy compared to traditional approaches. The method shows robust performance across different imaging protocols and patient populations.\nThe proposed framework represents a significant advancement in brain tumor differentiation, providing more accurate classification that could improve treatment planning and patient outcomes."
  },
  {
    "objectID": "publication/2018-01-01-primary-cns-lymphoma-sparse-representation/index.html",
    "href": "publication/2018-01-01-primary-cns-lymphoma-sparse-representation/index.html",
    "title": "Primary Central Nervous System Lymphoma and Glioblastoma Image Differentiation Based on Sparse Representation System",
    "section": "",
    "text": "Accurate differentiation between primary central nervous system lymphoma (PCNSL) and glioblastoma is essential for appropriate treatment selection, as these conditions have distinct therapeutic approaches and prognoses. Traditional diagnostic methods often rely on subjective visual assessment, which can lead to inconsistent results. In this work, we propose a sparse representation-based system for automated differentiation of PCNSL from glioblastoma.\nOur approach leverages sparse representation theory to capture the underlying structure of brain tumor imaging patterns. The framework consists of three main components: (1) a sparse coding module that learns discriminative representations, (2) a dictionary learning mechanism that captures tumor-specific features, and (3) a classification module that provides automated differentiation.\nWe evaluate our method on a comprehensive dataset of PCNSL and glioblastoma cases with confirmed diagnoses. Experimental results demonstrate that our sparse representation approach significantly improves differentiation accuracy compared to traditional methods. The system shows robust performance across different imaging protocols and provides interpretable results.\nThe proposed framework represents a significant advancement in brain tumor classification, providing more accurate and reliable differentiation that could improve clinical decision-making and patient outcomes."
  },
  {
    "objectID": "publication/2018-01-01-primary-cns-lymphoma-sparse-representation/index.html#abstract",
    "href": "publication/2018-01-01-primary-cns-lymphoma-sparse-representation/index.html#abstract",
    "title": "Primary Central Nervous System Lymphoma and Glioblastoma Image Differentiation Based on Sparse Representation System",
    "section": "",
    "text": "Accurate differentiation between primary central nervous system lymphoma (PCNSL) and glioblastoma is essential for appropriate treatment selection, as these conditions have distinct therapeutic approaches and prognoses. Traditional diagnostic methods often rely on subjective visual assessment, which can lead to inconsistent results. In this work, we propose a sparse representation-based system for automated differentiation of PCNSL from glioblastoma.\nOur approach leverages sparse representation theory to capture the underlying structure of brain tumor imaging patterns. The framework consists of three main components: (1) a sparse coding module that learns discriminative representations, (2) a dictionary learning mechanism that captures tumor-specific features, and (3) a classification module that provides automated differentiation.\nWe evaluate our method on a comprehensive dataset of PCNSL and glioblastoma cases with confirmed diagnoses. Experimental results demonstrate that our sparse representation approach significantly improves differentiation accuracy compared to traditional methods. The system shows robust performance across different imaging protocols and provides interpretable results.\nThe proposed framework represents a significant advancement in brain tumor classification, providing more accurate and reliable differentiation that could improve clinical decision-making and patient outcomes."
  },
  {
    "objectID": "publication/2023-09-01-robust-segmentation-topology-violation/index.html",
    "href": "publication/2023-09-01-robust-segmentation-topology-violation/index.html",
    "title": "Robust Segmentation via Topology Violation Detection and Feature Synthesis",
    "section": "",
    "text": "Medical image segmentation often requires maintaining anatomical topology and structural consistency, which can be challenging when dealing with complex or noisy data. Traditional segmentation methods may produce topologically incorrect results that violate anatomical constraints. In this work, we propose a novel approach for robust segmentation that combines topology violation detection with feature synthesis.\nOur method introduces a topology-aware framework that detects and corrects topological violations during segmentation. The framework consists of three key components: (1) a topology violation detection module that identifies structural inconsistencies, (2) a feature synthesis module that generates corrective features, and (3) a robust segmentation network that maintains anatomical topology.\nWe evaluate our approach on multiple medical imaging datasets, including brain MRI and fetal ultrasound. Experimental results demonstrate that our topology-aware approach significantly improves segmentation robustness compared to traditional methods. The method shows excellent performance in maintaining anatomical consistency and reducing topological errors.\nThe proposed framework represents a significant advancement in medical image segmentation, providing more anatomically accurate results that could improve clinical applications and research studies."
  },
  {
    "objectID": "publication/2023-09-01-robust-segmentation-topology-violation/index.html#abstract",
    "href": "publication/2023-09-01-robust-segmentation-topology-violation/index.html#abstract",
    "title": "Robust Segmentation via Topology Violation Detection and Feature Synthesis",
    "section": "",
    "text": "Medical image segmentation often requires maintaining anatomical topology and structural consistency, which can be challenging when dealing with complex or noisy data. Traditional segmentation methods may produce topologically incorrect results that violate anatomical constraints. In this work, we propose a novel approach for robust segmentation that combines topology violation detection with feature synthesis.\nOur method introduces a topology-aware framework that detects and corrects topological violations during segmentation. The framework consists of three key components: (1) a topology violation detection module that identifies structural inconsistencies, (2) a feature synthesis module that generates corrective features, and (3) a robust segmentation network that maintains anatomical topology.\nWe evaluate our approach on multiple medical imaging datasets, including brain MRI and fetal ultrasound. Experimental results demonstrate that our topology-aware approach significantly improves segmentation robustness compared to traditional methods. The method shows excellent performance in maintaining anatomical consistency and reducing topological errors.\nThe proposed framework represents a significant advancement in medical image segmentation, providing more anatomically accurate results that could improve clinical applications and research studies."
  },
  {
    "objectID": "publication/2019-10-01-encoding-ct-anatomy-knowledge/index.html",
    "href": "publication/2019-10-01-encoding-ct-anatomy-knowledge/index.html",
    "title": "Encoding CT Anatomy Knowledge for Unpaired Chest X-ray Image Decomposition",
    "section": "",
    "text": "Chest X-ray image decomposition is crucial for improving diagnostic accuracy by separating different anatomical structures. However, traditional approaches often struggle with maintaining anatomical accuracy, particularly when dealing with unpaired data. In this work, we propose a novel approach that encodes CT anatomy knowledge to guide unpaired chest X-ray image decomposition.\nOur method leverages the rich anatomical information available in CT scans to guide the decomposition of chest X-ray images, even without paired training data. The framework consists of three key components: (1) an anatomy knowledge encoder that learns structural information from CT scans, (2) an unpaired learning framework that transfers anatomical knowledge to X-ray domain, and (3) a decomposition module that produces anatomically accurate component images.\nWe evaluate our approach on a large dataset of chest X-ray images with expert annotations. Experimental results demonstrate that our anatomy-guided approach significantly improves decomposition quality compared to existing methods. The method shows excellent performance in maintaining anatomical accuracy while providing clear separation of different tissue types.\nThe proposed framework represents a significant advancement in chest X-ray processing, providing more accurate decomposition that could improve diagnostic interpretation and clinical decision-making."
  },
  {
    "objectID": "publication/2019-10-01-encoding-ct-anatomy-knowledge/index.html#abstract",
    "href": "publication/2019-10-01-encoding-ct-anatomy-knowledge/index.html#abstract",
    "title": "Encoding CT Anatomy Knowledge for Unpaired Chest X-ray Image Decomposition",
    "section": "",
    "text": "Chest X-ray image decomposition is crucial for improving diagnostic accuracy by separating different anatomical structures. However, traditional approaches often struggle with maintaining anatomical accuracy, particularly when dealing with unpaired data. In this work, we propose a novel approach that encodes CT anatomy knowledge to guide unpaired chest X-ray image decomposition.\nOur method leverages the rich anatomical information available in CT scans to guide the decomposition of chest X-ray images, even without paired training data. The framework consists of three key components: (1) an anatomy knowledge encoder that learns structural information from CT scans, (2) an unpaired learning framework that transfers anatomical knowledge to X-ray domain, and (3) a decomposition module that produces anatomically accurate component images.\nWe evaluate our approach on a large dataset of chest X-ray images with expert annotations. Experimental results demonstrate that our anatomy-guided approach significantly improves decomposition quality compared to existing methods. The method shows excellent performance in maintaining anatomical accuracy while providing clear separation of different tissue types.\nThe proposed framework represents a significant advancement in chest X-ray processing, providing more accurate decomposition that could improve diagnostic interpretation and clinical decision-making."
  },
  {
    "objectID": "publication/2017-09-01-brain-tumor-segmentation-adversarial-network/index.html",
    "href": "publication/2017-09-01-brain-tumor-segmentation-adversarial-network/index.html",
    "title": "Brain Tumor Segmentation Using an Adversarial Network",
    "section": "",
    "text": "Brain tumor segmentation is a critical task in medical image analysis, with applications ranging from treatment planning to outcome prediction. Traditional segmentation methods often struggle with the complex and heterogeneous nature of brain tumors. In this work, we propose a novel adversarial network approach for brain tumor segmentation that leverages the power of generative adversarial networks to improve segmentation accuracy.\nOur method introduces an adversarial training framework that consists of a segmentation network and a discriminator network. The segmentation network learns to produce accurate tumor masks, while the discriminator network ensures that the generated masks are realistic and consistent with expert annotations. The framework consists of three main components: (1) a segmentation generator that produces tumor masks, (2) a discriminator that evaluates mask quality, and (3) an adversarial training mechanism that improves both networks.\nWe evaluate our approach on the BraTS challenge dataset with expert annotations. Experimental results demonstrate that our adversarial network approach significantly improves segmentation accuracy compared to traditional methods. The method shows excellent performance in handling the challenging aspects of brain tumor segmentation, including boundary ambiguity and tissue heterogeneity.\nThe proposed framework represents a significant advancement in brain tumor segmentation, providing more accurate results that could improve treatment planning and patient outcomes."
  },
  {
    "objectID": "publication/2017-09-01-brain-tumor-segmentation-adversarial-network/index.html#abstract",
    "href": "publication/2017-09-01-brain-tumor-segmentation-adversarial-network/index.html#abstract",
    "title": "Brain Tumor Segmentation Using an Adversarial Network",
    "section": "",
    "text": "Brain tumor segmentation is a critical task in medical image analysis, with applications ranging from treatment planning to outcome prediction. Traditional segmentation methods often struggle with the complex and heterogeneous nature of brain tumors. In this work, we propose a novel adversarial network approach for brain tumor segmentation that leverages the power of generative adversarial networks to improve segmentation accuracy.\nOur method introduces an adversarial training framework that consists of a segmentation network and a discriminator network. The segmentation network learns to produce accurate tumor masks, while the discriminator network ensures that the generated masks are realistic and consistent with expert annotations. The framework consists of three main components: (1) a segmentation generator that produces tumor masks, (2) a discriminator that evaluates mask quality, and (3) an adversarial training mechanism that improves both networks.\nWe evaluate our approach on the BraTS challenge dataset with expert annotations. Experimental results demonstrate that our adversarial network approach significantly improves segmentation accuracy compared to traditional methods. The method shows excellent performance in handling the challenging aspects of brain tumor segmentation, including boundary ambiguity and tissue heterogeneity.\nThe proposed framework represents a significant advancement in brain tumor segmentation, providing more accurate results that could improve treatment planning and patient outcomes."
  },
  {
    "objectID": "publication/2018-07-01-multi-scope-cnn-left-ventricle-segmentation/index.html",
    "href": "publication/2018-07-01-multi-scope-cnn-left-ventricle-segmentation/index.html",
    "title": "A Multi-Scope Convolutional Neural Network for Automatic Left Ventricle Segmentation from Magnetic Resonance Images: Deep-Learning at Multiple Scopes",
    "section": "",
    "text": "Automatic left ventricle segmentation from cardiac MRI is crucial for quantitative assessment of cardiac function. Traditional approaches often use single-scale analysis, which may miss important features at different spatial scales. In this work, we propose a multi-scope convolutional neural network approach that leverages information at multiple scales for improved left ventricle segmentation.\nOur method introduces a multi-scope architecture that processes cardiac images at different spatial resolutions simultaneously. The framework consists of three key components: (1) multiple scope modules that capture features at different scales, (2) a feature fusion mechanism that combines multi-scale information, and (3) a segmentation network that leverages the fused features for accurate delineation.\nWe evaluate our approach on cardiac MRI datasets with expert annotations. Experimental results demonstrate that our multi-scope approach significantly improves segmentation accuracy compared to single-scale methods. The method shows excellent performance in capturing both fine details and global structure of the left ventricle.\nThe proposed framework represents a significant advancement in cardiac image analysis, providing more accurate segmentation that could improve cardiac function assessment and clinical diagnosis."
  },
  {
    "objectID": "publication/2018-07-01-multi-scope-cnn-left-ventricle-segmentation/index.html#abstract",
    "href": "publication/2018-07-01-multi-scope-cnn-left-ventricle-segmentation/index.html#abstract",
    "title": "A Multi-Scope Convolutional Neural Network for Automatic Left Ventricle Segmentation from Magnetic Resonance Images: Deep-Learning at Multiple Scopes",
    "section": "",
    "text": "Automatic left ventricle segmentation from cardiac MRI is crucial for quantitative assessment of cardiac function. Traditional approaches often use single-scale analysis, which may miss important features at different spatial scales. In this work, we propose a multi-scope convolutional neural network approach that leverages information at multiple scales for improved left ventricle segmentation.\nOur method introduces a multi-scope architecture that processes cardiac images at different spatial resolutions simultaneously. The framework consists of three key components: (1) multiple scope modules that capture features at different scales, (2) a feature fusion mechanism that combines multi-scale information, and (3) a segmentation network that leverages the fused features for accurate delineation.\nWe evaluate our approach on cardiac MRI datasets with expert annotations. Experimental results demonstrate that our multi-scope approach significantly improves segmentation accuracy compared to single-scale methods. The method shows excellent performance in capturing both fine details and global structure of the left ventricle.\nThe proposed framework represents a significant advancement in cardiac image analysis, providing more accurate segmentation that could improve cardiac function assessment and clinical diagnosis."
  },
  {
    "objectID": "publication/2021-12-01-federated-covid-19-lung-abnormalities/index.html",
    "href": "publication/2021-12-01-federated-covid-19-lung-abnormalities/index.html",
    "title": "Federated Deep Learning for Detecting COVID-19 Lung Abnormalities in CT: A Privacy-Preserving Multinational Validation Study",
    "section": "",
    "text": "The COVID-19 pandemic has highlighted the critical need for rapid and accurate diagnostic tools, with chest CT imaging playing a crucial role in detecting lung abnormalities. However, developing robust AI models for COVID-19 detection requires large, diverse datasets that are often distributed across multiple institutions, raising privacy and data sharing concerns. In this work, we present a federated learning approach for detecting COVID-19 lung abnormalities in CT images across multiple institutions while preserving data privacy.\nOur federated learning framework enables collaborative model training without sharing raw patient data between institutions. The approach consists of three key components: (1) a distributed training protocol that allows local model updates, (2) a secure aggregation mechanism that combines model parameters without exposing individual data, and (3) a validation framework that ensures model performance across diverse populations.\nWe evaluate our approach on a multinational dataset involving multiple institutions across different countries. Experimental results demonstrate that federated learning achieves comparable performance to centralized training while maintaining data privacy. The method shows robust performance across different populations and imaging protocols, providing a scalable solution for COVID-19 detection.\nThe proposed framework represents a significant advancement in privacy-preserving medical AI, demonstrating the potential for collaborative model development while maintaining strict privacy standards."
  },
  {
    "objectID": "publication/2021-12-01-federated-covid-19-lung-abnormalities/index.html#abstract",
    "href": "publication/2021-12-01-federated-covid-19-lung-abnormalities/index.html#abstract",
    "title": "Federated Deep Learning for Detecting COVID-19 Lung Abnormalities in CT: A Privacy-Preserving Multinational Validation Study",
    "section": "",
    "text": "The COVID-19 pandemic has highlighted the critical need for rapid and accurate diagnostic tools, with chest CT imaging playing a crucial role in detecting lung abnormalities. However, developing robust AI models for COVID-19 detection requires large, diverse datasets that are often distributed across multiple institutions, raising privacy and data sharing concerns. In this work, we present a federated learning approach for detecting COVID-19 lung abnormalities in CT images across multiple institutions while preserving data privacy.\nOur federated learning framework enables collaborative model training without sharing raw patient data between institutions. The approach consists of three key components: (1) a distributed training protocol that allows local model updates, (2) a secure aggregation mechanism that combines model parameters without exposing individual data, and (3) a validation framework that ensures model performance across diverse populations.\nWe evaluate our approach on a multinational dataset involving multiple institutions across different countries. Experimental results demonstrate that federated learning achieves comparable performance to centralized training while maintaining data privacy. The method shows robust performance across different populations and imaging protocols, providing a scalable solution for COVID-19 detection.\nThe proposed framework represents a significant advancement in privacy-preserving medical AI, demonstrating the potential for collaborative model development while maintaining strict privacy standards."
  },
  {
    "objectID": "publication/2022-01-15-breast-tumor-classification-mri-us/index.html",
    "href": "publication/2022-01-15-breast-tumor-classification-mri-us/index.html",
    "title": "Breast Tumor Classification based on MRI-US Images by Disentangling Modality Features",
    "section": "",
    "text": "Breast cancer diagnosis often requires information from multiple imaging modalities, with MRI and ultrasound providing complementary information about tumor characteristics. However, effectively combining information from different modalities remains challenging due to their inherent differences in image characteristics and feature representations. In this work, we propose a novel approach for breast tumor classification that disentangles modality-specific features while leveraging complementary information.\nOur framework introduces a modality disentanglement module that separates modality-specific and modality-invariant features from MRI and ultrasound images. The approach consists of three key components: (1) a modality-specific encoder that extracts unique features from each modality, (2) a modality-invariant encoder that captures shared characteristics, and (3) a fusion module that combines both feature types for final classification.\nWe evaluate our method on a comprehensive dataset of breast tumor cases with both MRI and ultrasound imaging. Experimental results demonstrate that our modality disentanglement approach significantly improves classification accuracy compared to traditional multi-modal fusion methods. The method shows robust performance across different tumor types and imaging conditions.\nThe proposed framework represents a significant advancement in multi-modal breast cancer diagnosis, providing more accurate classification while maintaining interpretability of modality-specific contributions."
  },
  {
    "objectID": "publication/2022-01-15-breast-tumor-classification-mri-us/index.html#abstract",
    "href": "publication/2022-01-15-breast-tumor-classification-mri-us/index.html#abstract",
    "title": "Breast Tumor Classification based on MRI-US Images by Disentangling Modality Features",
    "section": "",
    "text": "Breast cancer diagnosis often requires information from multiple imaging modalities, with MRI and ultrasound providing complementary information about tumor characteristics. However, effectively combining information from different modalities remains challenging due to their inherent differences in image characteristics and feature representations. In this work, we propose a novel approach for breast tumor classification that disentangles modality-specific features while leveraging complementary information.\nOur framework introduces a modality disentanglement module that separates modality-specific and modality-invariant features from MRI and ultrasound images. The approach consists of three key components: (1) a modality-specific encoder that extracts unique features from each modality, (2) a modality-invariant encoder that captures shared characteristics, and (3) a fusion module that combines both feature types for final classification.\nWe evaluate our method on a comprehensive dataset of breast tumor cases with both MRI and ultrasound imaging. Experimental results demonstrate that our modality disentanglement approach significantly improves classification accuracy compared to traditional multi-modal fusion methods. The method shows robust performance across different tumor types and imaging conditions.\nThe proposed framework represents a significant advancement in multi-modal breast cancer diagnosis, providing more accurate classification while maintaining interpretability of modality-specific contributions."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "ÊùéÊ≥ΩÊ¶â",
    "section": "",
    "text": "Zeju Li (He/Him)\n\nAssociate Professor\nCollege of Biomedical Engineering\nFudan University\n¬†\nOffice Location\nRoom C2008, No.¬†2 Interdisciplinary Research Building\nJiangwan Campus, Fudan University\nShanghai, China\n¬†\nEmail\nüìß Work: zejuli(at)fudan.edu.cn\nüìß Personal: lizeju79(at)sina.cn"
  },
  {
    "objectID": "about/index.html#about-me",
    "href": "about/index.html#about-me",
    "title": "ÊùéÊ≥ΩÊ¶â",
    "section": "About Me",
    "text": "About Me\n\n\nZeju Li‚Äôs research focuses on machine learning and its applications in medical image processing, with an emphasis on neuroimaging analysis and addressing clinical problems.\nHe has published over 30 academic papers, many of which appeared in TMI and MICCAI.\nHis work has garnered over 4,000 citations on Google Scholar. He has won championships in the Huawei AI Challenge and the MICCAI Challenge, and serves as an area chair for conferences such as MICCAI and MIDL.\n\n\nZeju was a Post-Doctoral Researcher in the FMRIB Analysis Group at the University of Oxford before joining Fudan in February 2025.\nHe obtained PhD in Computing from BioMedIA Group. He spent time in Institute of Computing Technology and Huawei Noah‚Äôs Ark Lab (London).\n\n\nPh.D.¬†in Computing ‚àô Imperial College London ‚àô 2023\n\n\nM.S. in Biomedical Engineering ‚àô Fudan University ‚àô 2023\n\n\nB.S. in Electronic Engineering ‚àô Fudan University ‚àô 2023\n\n\n\n\n\nTo Prospective Student: I have multiple positions available (PhD, Master‚Äôs, interns, and Post-doc) at Fudan University. If you are interested, please read this). If you cannot read Chinese but are still interested, feel free to contact me directly.\n\n\n\nÊùéÊ≥ΩÊ¶âÔºåÁé∞Â§çÊó¶Â§ßÂ≠¶ÁîüÁâ©ÂåªÂ≠¶Â∑•Á®ã‰∏éÊäÄÊúØÂàõÊñ∞Â≠¶Èô¢ÈùíÂπ¥Á†îÁ©∂ÂëòÔºåËé∑ÂõΩÂÆ∂È´òÂ±ÇÊ¨°ÈùíÂπ¥‰∫∫ÊâçÈ°πÁõÆÂíå‰∏äÊµ∑Â∏ÇÈ´òÂ±ÇÊ¨°ÈùíÂπ¥‰∫∫ÊâçÈ°πÁõÆËµÑÂä©„ÄÇ ‰ªñ‰∫é2023Âπ¥1ÊúàÂú®Â∏ùÂõΩÁêÜÂ∑•Â≠¶Èô¢ComputingÁ≥ªËé∑ÂæóÂçöÂ£´Â≠¶‰ΩçÔºåÊ≠§ÂâçÊòØÁâõÊ¥•Â§ßÂ≠¶FMRIB CentreÁöÑÂçöÂ£´ÂêéÁ†îÁ©∂Âëò„ÄÇÊùéÊ≥ΩÊ¶âÁöÑÁ†îÁ©∂ÊñπÂêëÈõÜ‰∏≠Âú®Êú∫Âô®Â≠¶‰π†ÂèäÂÖ∂Âú®ÂåªÂ≠¶ÂõæÂÉèÂ§ÑÁêÜ‰∏≠ÁöÑÂ∫îÁî®Ôºå‰æßÈáçÁ•ûÁªèÂΩ±ÂÉèÂàÜÊûêÂíåËß£ÂÜ≥‰∏¥Â∫äÈóÆÈ¢òÔºåÂÖ∂ÂèëË°®Â≠¶ÊúØËÆ∫Êñá30‰ΩôÁØáÔºåÂÖ∂‰∏≠Â§öÁØáÂèëË°®‰∫éTMIÂíåMICCAIÔºõGoogle ScholarÂºïÁî®Ë∂ÖËøá4000Ê¨°ÔºåÊõæËé∑ÂæóÂçé‰∏∫AIÊåëÊàòËµõÂíåMICCAIÊåëÊàòËµõÂÜ†ÂÜõÔºåÂπ∂ÊãÖ‰ªªMICCAI/MIDLÁ≠âÈ¢ÜÂüü‰ºöËÆÆÁöÑÈ¢ÜÂüü‰∏ªÂ∏≠„ÄÇ"
  },
  {
    "objectID": "about/index.html#latest-update",
    "href": "about/index.html#latest-update",
    "title": "ÊùéÊ≥ΩÊ¶â",
    "section": "Latest Update",
    "text": "Latest Update\n\n\n\n\n\n\n\n\n\n\n\nGenerative Machine Learning Models in Medical Image Computing\n\n\nA comprehensive exploration of generative modeling techniques tailored to the unique demands of medical imaging.\n\n\n\n\n\n\n\n\n\n\n\n\nDehazing Echocardiography Challenge 2025\n\n\nWe are organing a challenge in conjunction with MICCAI 2025.\n\n\n\n\n\n\n\n\n\n\n\n\nMICS 2025 at Cixi\n\n\nThe largest medical image analysis seminar in China\n\n\n\n\n\nNo matching items\n\nSee all ‚Üí\n\nOther Interests\nI enjoy building beautiful things, such as photography, calligraphy and making videos. This website is a weekend project built with Quarto and Cursor, inspired by Silvia.\n\n\n\nThis is a picture I took while traveling around Morro Bay"
  },
  {
    "objectID": "about/index.html#other-interests",
    "href": "about/index.html#other-interests",
    "title": "ÊùéÊ≥ΩÊ¶â",
    "section": "Other Interests",
    "text": "Other Interests\nI enjoy building beautiful things, such as photography, calligraphy and making videos. This website is a weekend project built with Quarto and Cursor, inspired by Silvia.\n\n\n\nThis is a picture I took while traveling around Morro Bay"
  },
  {
    "objectID": "index.html#associate-professor-at-fudan-university",
    "href": "index.html#associate-professor-at-fudan-university",
    "title": "Zeju Li, PhD",
    "section": "ASSOCIATE PROFESSOR AT FUDAN UNIVERSITY",
    "text": "ASSOCIATE PROFESSOR AT FUDAN UNIVERSITY\n\nI am a researcher at the intersection of machine learning and medical imaging. My research focuses on designing novel computational approaches for biomedical image analysis, with applications in image reconstruction, brain function analysis and disease diagnosis.\n\n\nLEARN MORE ABOUT ME ‚Üí"
  },
  {
    "objectID": "index.html#featured",
    "href": "index.html#featured",
    "title": "Zeju Li, PhD",
    "section": "Featured",
    "text": "Featured"
  },
  {
    "objectID": "index.html#hiring",
    "href": "index.html#hiring",
    "title": "Zeju Li, PhD",
    "section": "Hiring",
    "text": "Hiring"
  },
  {
    "objectID": "news/2025-08-08-MICCAI2025/index.html",
    "href": "news/2025-08-08-MICCAI2025/index.html",
    "title": "Dehazing Echocardiography Challenge 2025",
    "section": "",
    "text": "The challenge is now online at link."
  },
  {
    "objectID": "news/2025-08-08-MICCAI2025/index.html#description",
    "href": "news/2025-08-08-MICCAI2025/index.html#description",
    "title": "Dehazing Echocardiography Challenge 2025",
    "section": "",
    "text": "The challenge is now online at link."
  },
  {
    "objectID": "news/2025-08-08-MICS/index.html",
    "href": "news/2025-08-08-MICS/index.html",
    "title": "MICS 2025 at Cixi",
    "section": "",
    "text": "This summer, I attended MICS in Cixi and gave a talk."
  },
  {
    "objectID": "news/2025-08-08-MICS/index.html#description",
    "href": "news/2025-08-08-MICS/index.html#description",
    "title": "MICS 2025 at Cixi",
    "section": "",
    "text": "This summer, I attended MICS in Cixi and gave a talk."
  },
  {
    "objectID": "team/weidong_guo/index.html",
    "href": "team/weidong_guo/index.html",
    "title": "Weidong Guo",
    "section": "",
    "text": "ËåÉ‰∏≠Áùø\n\nÂçöÂ£´ÁîüÁ†îÁ©∂Âëò\n¬†\nCollege of Biomedical Engineering\nFudan University\n¬†\nLab Period\nMay 2025 - Present\nüìß Email gwd200(at)mail.ustc.edu.cn"
  },
  {
    "objectID": "team/weidong_guo/index.html#about",
    "href": "team/weidong_guo/index.html#about",
    "title": "Weidong Guo",
    "section": "About",
    "text": "About\n\n\nWeidong Guo is a doctor student working on advanced computational approaches for neuroimaging.\n\n\nWeidong joined the lab in 2025 and is currently pursuing his Bachelor‚Äôs degree in Biomedical Engineering at the Fudan University.\n\n\nPh.D.¬†‚àô Fudan University ‚àô Expected 2029\n\n\nM.S. ‚àô University of Science and Technology of China ‚àô 2025\n\n\nB.S. ‚àô University of Science and Technology of China ‚àô 2022"
  },
  {
    "objectID": "team/zhongrui_fan/index.html",
    "href": "team/zhongrui_fan/index.html",
    "title": "Zhongrui Fan",
    "section": "",
    "text": "ËåÉ‰∏≠Áùø\n\nÊú¨ÁßëÁîüÁ†îÁ©∂Âëò\n¬†\nCollege of Biomedical Engineering\nFudan University\n¬†\nLab Period\nJun 2025 - Present\nüìß Email 22307130437(at)m.fudan.edu.cn"
  },
  {
    "objectID": "team/zhongrui_fan/index.html#about",
    "href": "team/zhongrui_fan/index.html#about",
    "title": "Zhongrui Fan",
    "section": "About",
    "text": "About\n\n\nZhongrui Fan is an undergraduate student working on generative model in medical imaging.\n\n\nZhongrui joined the lab in 2025 and is currently pursuing his Bachelor‚Äôs degree in Biomedical Engineering at the Fudan University.\n\n\nB.S. ‚àô Fudan University ‚àô Expected 2026"
  },
  {
    "objectID": "team/wutong_li/index.html",
    "href": "team/wutong_li/index.html",
    "title": "Wutong Li",
    "section": "",
    "text": "ÊùéÊ¢ßÊ°ê\n\nÂçöÂ£´ÁîüÁ†îÁ©∂Âëò\n¬†\nCollege of Biomedical Engineering\nFudan University\n¬†\nLab Period\nSep 2022 - Present\nüìß Email wtli22(at)m.fudan.edu.cn"
  },
  {
    "objectID": "team/wutong_li/index.html#about",
    "href": "team/wutong_li/index.html#about",
    "title": "Wutong Li",
    "section": "About",
    "text": "About\n\n\nWutong Li is a doctor student working on domain robust learning.\n\n\nWutong joined the lab in 2022 and is currently pursuing his doctoral degree in Biomedical Engineering at Fudan University.\n\n\nPh.D.¬†‚àô Fudan University ‚àô Expected 2027\n\n\nB.S. ‚àô Nankai University ‚àô 2022"
  },
  {
    "objectID": "team/xinyu_jia/index.html",
    "href": "team/xinyu_jia/index.html",
    "title": "Xinyu Jia",
    "section": "",
    "text": "Ë¥æÊñ∞ÂÆá\n\nÊú¨ÁßëÁîüÁ†îÁ©∂Âëò\n¬†\nCollege of Biomedical Engineering\nFudan University\n¬†\nLab Period\nMay 2025 - Present\nüìß Email 22307130086(at)m.fudan.edu.cn"
  },
  {
    "objectID": "team/xinyu_jia/index.html#about",
    "href": "team/xinyu_jia/index.html#about",
    "title": "Xinyu Jia",
    "section": "About",
    "text": "About\n\n\nXinyu Jia is an undergraduate student working on improving model generation based on generative models.\n\n\nXinyu joined the lab in 2025 and is currently pursuing his Bachelor‚Äôs degree in Electronic Engineering at the Fudan University.\n\n\nB.S. ‚àô Fudan University ‚àô Expected 2026"
  },
  {
    "objectID": "team/yi_yang/index.html",
    "href": "team/yi_yang/index.html",
    "title": "Yi Yang",
    "section": "",
    "text": "Êù®ÊØÖ\n\nÁ°ïÂ£´ÁîüÁ†îÁ©∂Âëò\n¬†\nCollege of Biomedical Engineering\nFudan University\n¬†\nLab Period\nSep 2025 - Present\nüìß Email 25213090038(at)m.fudan.edu.cn"
  },
  {
    "objectID": "team/yi_yang/index.html#about",
    "href": "team/yi_yang/index.html#about",
    "title": "Yi Yang",
    "section": "About",
    "text": "About\n\n\nYi Yang is a master‚Äôs student working on brain signal generation.\n\n\nYi joined the lab in 2025 and is currently pursuing his Master‚Äôs degree in Biomedical Engineering at the Fudan University.\n\n\nM.S. ‚àô Fudan University ‚àô Expected 2028\n\n\nB.S. ‚àô Xiamen University ‚àô Expected 2025"
  },
  {
    "objectID": "team/lei_liu/index.html",
    "href": "team/lei_liu/index.html",
    "title": "Lei Liu",
    "section": "",
    "text": "ÂàòÁ£ä\n\nÂçöÂ£´ÁîüÁ†îÁ©∂Âëò\n¬†\nCollege of Biomedical Engineering\nFudan University\n¬†\nLab Period\nJun 2025 - Present\nüìß Email liulei.yossarian(at)gmail.com"
  },
  {
    "objectID": "team/lei_liu/index.html#about",
    "href": "team/lei_liu/index.html#about",
    "title": "Lei Liu",
    "section": "About",
    "text": "About\n\n\nLei Liu is a doctor student working on medical LLMs.\n\n\nLei joined the lab in 2025 and is currently pursuing his doctoral degree in Biomedical Engineering at Fudan University.\n\n\nPh.D.¬†‚àô Fudan University ‚àô Expected 2029\n\n\nM.S. ‚àô University of Electronic Science and Technology of China ‚àô 2023\n\n\nB.S. ‚àô University of Electronic Science and Technology of China ‚àô 2020"
  },
  {
    "objectID": "teaching/2025-08-08-AI/index.html",
    "href": "teaching/2025-08-08-AI/index.html",
    "title": "AI Foundations 2025",
    "section": "",
    "text": "I am teaching an AI course for a general audience, designed for students from all majors."
  },
  {
    "objectID": "teaching/2025-08-08-AI/index.html#ai-foundations",
    "href": "teaching/2025-08-08-AI/index.html#ai-foundations",
    "title": "AI Foundations 2025",
    "section": "AI Foundations",
    "text": "AI Foundations\nIn this course, I will cover some basic concepts of AI."
  },
  {
    "objectID": "teaching/2025-08-08-AI/index.html#recommended-reading-materials",
    "href": "teaching/2025-08-08-AI/index.html#recommended-reading-materials",
    "title": "AI Foundations 2025",
    "section": "Recommended Reading Materials",
    "text": "Recommended Reading Materials\n\nMathematics for Machine Learning.\nDive into Deep Learning."
  },
  {
    "objectID": "teaching/2025-08-08-Pattern Recognition/index.html",
    "href": "teaching/2025-08-08-Pattern Recognition/index.html",
    "title": "Pattern Recognition 2025",
    "section": "",
    "text": "I am teaching an pattern regnition course for BME Ph.D.¬†students."
  },
  {
    "objectID": "teaching/2025-08-08-Pattern Recognition/index.html#pattern-recognition",
    "href": "teaching/2025-08-08-Pattern Recognition/index.html#pattern-recognition",
    "title": "Pattern Recognition 2025",
    "section": "Pattern Recognition",
    "text": "Pattern Recognition\nIn this course, I will cover some advanced concepts of AI."
  },
  {
    "objectID": "teaching/2025-08-08-Pattern Recognition/index.html#recommended-reading-materials",
    "href": "teaching/2025-08-08-Pattern Recognition/index.html#recommended-reading-materials",
    "title": "Pattern Recognition 2025",
    "section": "Recommended Reading Materials",
    "text": "Recommended Reading Materials\n\nMathematics for Machine Learning.\nDive into Deep Learning."
  },
  {
    "objectID": "alumni/bowen_guo/index.html",
    "href": "alumni/bowen_guo/index.html",
    "title": "Bowen Guo",
    "section": "",
    "text": "ÈÉ≠ÂçöÊñá\n\nÊú¨ÁßëÁîüÁ†îÁ©∂Âëò\n¬†\nCollege of Biomedical Engineering\nFudan University\n¬†\nLab Period\nFeb 2025 - Jul 2025\nüìß Email 21307130160(at)m.fudan.edu.cn"
  },
  {
    "objectID": "alumni/bowen_guo/index.html#about",
    "href": "alumni/bowen_guo/index.html#about",
    "title": "Bowen Guo",
    "section": "About",
    "text": "About\n\n\nBowen Guo was an undergraduate student working on multi-modal generative model.\n\n\nBowen joined the lab in 2025 and later pursued a Ph.D.¬†at Fudan University after leaving us.\n\n\nB.S. ‚àô Fudan University ‚àô 2025"
  }
]